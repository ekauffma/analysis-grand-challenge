{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d27aa765-c7b7-4697-bae7-2ab0d1613ccd",
   "metadata": {},
   "source": [
    "# ttbar Analysis - Jet-Parton Assignment Training\n",
    "\n",
    "This is the training notebook for the jet-parton assignment task. The goal is to associate the leading four jets in each event to their associated parent particles. We are trying to assign jets according to the labels in the diagram below:\n",
    "\n",
    "<img src=\"utils/ttbar_labels.png\" alt=\"ttbar_labels\" width=\"500\"/>\n",
    "\n",
    "\n",
    "<mark>top<sub>lepton</sub></mark> and <mark>top<sub>hadron</sub></mark> jets do not necessarily correspond to top/antitop, respectively. The <mark>top<sub>lepton</sub></mark> jet is defined as having a lepton/neutrino pair as cousins, where the <mark>top<sub>hadron</sub></mark> jet is defined as having two jets as cousins. The <mark>W</mark> jets are not distinguished from each other.\n",
    "\n",
    "The strategy for solving this problem is to train a boosted decision tree to find the correct assignments for each jet. Since we consider four jets per event with three unique labels (<mark>W</mark>, <mark>top<sub>lepton</sub></mark>, and <mark>top<sub>hadron</sub></mark>), there are twelve possible combinations of assignments:\n",
    "\n",
    "<img src=\"utils/jetcombinations.png\" alt=\"jetcombinations\" width=\"700\"/>\n",
    "\n",
    "The combination with the highest BDT score will be selected for each event.\n",
    "____\n",
    "\n",
    "The workflow for this training notebook is outlined as follows:\n",
    "* Load data and calculate training features and labels using `coffea`/`dask`\n",
    "* Optimize BDT using $n$-fold cross-validation and track using `mlflow`\n",
    "* Register best model in `mlflow` model repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37255f8f-4c05-426f-aaaf-86907762c585",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from coffea.nanoevents import NanoAODSchema\n",
    "from coffea import processor\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import hist\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import uproot\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import utils\n",
    "\n",
    "# ML-related imports\n",
    "from dask.distributed import Client\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import ParameterSampler, train_test_split, KFold, cross_validate\n",
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow.tracking import MlflowClient\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9614d07-c1a3-49d5-96cd-c68114d4d034",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### GLOBAL CONFIGURATION\n",
    "\n",
    "# input files per process, set to e.g. 10 (smaller number = faster, want to use larger number for training)\n",
    "N_FILES_MAX_PER_SAMPLE = 5\n",
    "\n",
    "# set to True for DaskExecutor\n",
    "USE_DASK_PROCESSING = True\n",
    "\n",
    "# number of cores if using FuturesExecutor\n",
    "NUM_CORES = 4\n",
    "\n",
    "# chunk size to use\n",
    "CHUNKSIZE = 100_000\n",
    "\n",
    "# analysis facility: set to \"coffea_casa\" for coffea-casa environments, \"EAF\" for FNAL, \"local\" for local setups\n",
    "AF = \"coffea_casa\"\n",
    "\n",
    "\n",
    "### MACHINE LEARNING OPTIONS\n",
    "\n",
    "# enable Dask (whether to use dask for hyperparameter optimization. currently does not work)\n",
    "USE_DASK_ML = False\n",
    "\n",
    "# enable MLFlow logging (to store metrics and models of hyperparameter optimization trials)\n",
    "USE_MLFLOW = True\n",
    "\n",
    "# enable MLFlow model logging/registering\n",
    "MODEL_LOGGING = True\n",
    "MODEL_REGISTERING = True\n",
    "\n",
    "# enter generated mlflow tracking token (temporary solution) https://wiki.ncsa.illinois.edu/display/NCSASoftware/MLFlow+at+NCSA\n",
    "MLFLOW_TRACKING_TOKEN = \"\"\n",
    "\n",
    "# number of folds for cross-validation\n",
    "N_FOLD = 2\n",
    "\n",
    "# number of trials (per model) for hyperparameter optimization. Total number of trials will be 2*N_TRIALS\n",
    "N_TRIALS = 5\n",
    "\n",
    "# name to use for registering model\n",
    "MODEL_NAME = \"reconstruction_bdt_xgb\"\n",
    "\n",
    "# number of events to use for training (more results in higher efficiency, but slower to train)\n",
    "N_EVENTS_TRAIN = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250db8f8-1248-405a-9f02-5beb926f91cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get dictionaries for permutation indices, associated labels, and evaluation matrices\n",
    "# permutation indices correspond to the different possible combinations of jets in an event for correspondence \n",
    "# with the W boson, the top quark on the side of hadronic decay, and the top quark on the side of leptonic decay\n",
    "# evaluation matrix is used to calculate the fraction of matches correct within an event\n",
    "permutations_dict, labels_dict, evaluation_matrices = utils.get_permutations_dict(4, \n",
    "                                                                                  include_labels=True, \n",
    "                                                                                  include_eval_mat=True)\n",
    "evaluation_matrix = evaluation_matrices[4]\n",
    "print(evaluation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1a1ccb-c55b-4da6-9ea3-e2507a5db2f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## functions for calculating features and labels for the BDT\n",
    "def training_filter(jets, electrons, muons, genparts, even):\n",
    "    '''\n",
    "    Filters events down to training set and calculates jet-level labels\n",
    "    \n",
    "    Args:\n",
    "        jets: selected jets after region filter (and selecting leading four for each event)\n",
    "        electrons: selected electrons after region filter\n",
    "        muons: selected muons after region filter\n",
    "        genparts: selected genpart after region filter\n",
    "        even: whether the event is even-numbered (used to separate training events)\n",
    "    \n",
    "    Returns:\n",
    "        jets: selected jets after training filter\n",
    "        electrons: selected electrons after training filter\n",
    "        muons: selected muons after training filter\n",
    "        labels: labels of jets within an event (24=W, 6=top_hadron, -6=top_lepton)\n",
    "        even: whether the event is even-numbered\n",
    "    '''\n",
    "    #### filter genPart to valid matching candidates ####\n",
    "\n",
    "    # get rid of particles without parents\n",
    "    genpart_parent = genparts.distinctParent\n",
    "    genpart_filter = np.invert(ak.is_none(genpart_parent, axis=1))\n",
    "    genparts = genparts[genpart_filter]\n",
    "    genpart_parent = genparts.distinctParent\n",
    "\n",
    "    # ensure that parents are top quark or W\n",
    "    genpart_filter2 = ((np.abs(genpart_parent.pdgId)==6) | (np.abs(genpart_parent.pdgId)==24))\n",
    "    genparts = genparts[genpart_filter2]\n",
    "\n",
    "    # ensure particle itself is a quark\n",
    "    genpart_filter3 = ((np.abs(genparts.pdgId)<7) & (np.abs(genparts.pdgId)>0))\n",
    "    genparts = genparts[genpart_filter3]\n",
    "\n",
    "    # get rid of duplicates\n",
    "    genpart_filter4 = genparts.hasFlags(\"isLastCopy\")\n",
    "    genparts = genparts[genpart_filter4]\n",
    "            \n",
    "        \n",
    "    #### get jet-level labels and filter events to training set\n",
    "        \n",
    "    # match jets to nearest valid genPart candidate\n",
    "    nearest_genpart = jets.nearest(genparts, threshold=0.4)\n",
    "    nearest_parent = nearest_genpart.distinctParent # parent of matched particle\n",
    "    parent_pdgid = nearest_parent.pdgId # pdgId of parent particle\n",
    "    grandchild_pdgid = nearest_parent.distinctChildren.distinctChildren.pdgId # pdgId of particle's parent's grandchildren\n",
    "\n",
    "    grandchildren_flat = np.abs(ak.flatten(grandchild_pdgid,axis=-1)) # flatten innermost axis for convenience\n",
    "\n",
    "    # if particle has a cousin that is a lepton\n",
    "    has_lepton_cousin = (ak.sum(((grandchildren_flat%2==0) & (grandchildren_flat>10) & (grandchildren_flat<19)),\n",
    "                                axis=-1)>0)\n",
    "    # if particle has a cousin that is a neutrino\n",
    "    has_neutrino_cousin = (ak.sum(((grandchildren_flat%2==1) & (grandchildren_flat>10) & (grandchildren_flat<19)),\n",
    "                                  axis=-1)>0)\n",
    "\n",
    "    # if a particle has a lepton cousin and a neutrino cousin\n",
    "    has_both_cousins = ak.fill_none((has_lepton_cousin & has_neutrino_cousin), False).to_numpy()\n",
    "\n",
    "    # get labels from parent pdgId (fill none with 100 to filter out events with those jets)\n",
    "    labels = np.abs(ak.fill_none(parent_pdgid,100).to_numpy())\n",
    "    labels[has_both_cousins] = -6 # assign jets with both cousins as top_lepton (not necessarily antiparticle)\n",
    "\n",
    "    training_event_filter = (np.sum(labels,axis=1)==48) # events with a label sum of 48 have the correct particles\n",
    "            \n",
    "    # filter events\n",
    "    jets = jets[training_event_filter]\n",
    "    electrons = electrons[training_event_filter]\n",
    "    muons = muons[training_event_filter]\n",
    "    labels = labels[training_event_filter]\n",
    "    even = even[training_event_filter]\n",
    "    \n",
    "    return jets, electrons, muons, labels, even\n",
    "    \n",
    "\n",
    "def get_training_set(jets, electrons, muons, labels, permutations_dict, labels_dict):\n",
    "    '''\n",
    "    Calculate features for each of the 12 combinations per event and calculates combination-level labels\n",
    "    \n",
    "    Args:\n",
    "        jets: selected jets after training filter\n",
    "        electrons: selected electrons after training filter\n",
    "        muons: selected muons after training filter\n",
    "        labels: jet-level labels output by training_filter\n",
    "    \n",
    "    Returns:\n",
    "        features, labels (flattened to remove event level)\n",
    "    '''\n",
    "    \n",
    "    # calculate number of jets in each event\n",
    "    njet = ak.num(jets).to_numpy()\n",
    "    # don't consider every jet for events with high jet multiplicity\n",
    "    njet[njet>max(permutations_dict.keys())] = max(permutations_dict.keys())\n",
    "    # create awkward array of permutation indices\n",
    "    perms = ak.Array([permutations_dict[n] for n in njet])\n",
    "    perm_counts = ak.num(perms)\n",
    "    \n",
    "    #### calculate features ####\n",
    "    features = np.zeros((sum(perm_counts),20))\n",
    "    \n",
    "    # grab lepton info\n",
    "    leptons = ak.flatten(ak.concatenate((electrons, muons),axis=1),axis=-1)\n",
    "\n",
    "    feature_count = 0\n",
    "    \n",
    "    # delta R between top_lepton and lepton\n",
    "    features[:,0] = ak.flatten(np.sqrt((leptons.eta - jets[perms[...,3]].eta)**2 + \n",
    "                                       (leptons.phi - jets[perms[...,3]].phi)**2)).to_numpy()\n",
    "\n",
    "    \n",
    "    #delta R between the two W\n",
    "    features[:,1] = ak.flatten(np.sqrt((jets[perms[...,0]].eta - jets[perms[...,1]].eta)**2 + \n",
    "                                       (jets[perms[...,0]].phi - jets[perms[...,1]].phi)**2)).to_numpy()\n",
    "\n",
    "    #delta R between W and top_hadron\n",
    "    features[:,2] = ak.flatten(np.sqrt((jets[perms[...,0]].eta - jets[perms[...,2]].eta)**2 + \n",
    "                                       (jets[perms[...,0]].phi - jets[perms[...,2]].phi)**2)).to_numpy()\n",
    "    features[:,3] = ak.flatten(np.sqrt((jets[perms[...,1]].eta - jets[perms[...,2]].eta)**2 + \n",
    "                                       (jets[perms[...,1]].phi - jets[perms[...,2]].phi)**2)).to_numpy()\n",
    "\n",
    "    # combined mass of top_lepton and lepton\n",
    "    features[:,4] = ak.flatten((leptons + jets[perms[...,3]]).mass).to_numpy()\n",
    "\n",
    "    # combined mass of W\n",
    "    features[:,5] = ak.flatten((jets[perms[...,0]] + jets[perms[...,1]]).mass).to_numpy()\n",
    "\n",
    "    # combined mass of W and top_hadron\n",
    "    features[:,6] = ak.flatten((jets[perms[...,0]] + jets[perms[...,1]] + \n",
    "                                jets[perms[...,2]]).mass).to_numpy()\n",
    "    \n",
    "    # combined pT of W and top_hadron\n",
    "    features[:,7] = ak.flatten((jets[perms[...,0]] + jets[perms[...,1]] + \n",
    "                                jets[perms[...,2]]).pt).to_numpy()\n",
    "\n",
    "\n",
    "    # pt of every jet\n",
    "    features[:,8] = ak.flatten(jets[perms[...,0]].pt).to_numpy()\n",
    "    features[:,9] = ak.flatten(jets[perms[...,1]].pt).to_numpy()\n",
    "    features[:,10] = ak.flatten(jets[perms[...,2]].pt).to_numpy()\n",
    "    features[:,11] = ak.flatten(jets[perms[...,3]].pt).to_numpy()\n",
    "    \n",
    "    # btagCSVV2 of every jet\n",
    "    features[:,12] = ak.flatten(jets[perms[...,0]].btagCSVV2).to_numpy()\n",
    "    features[:,13] = ak.flatten(jets[perms[...,1]].btagCSVV2).to_numpy()\n",
    "    features[:,14] = ak.flatten(jets[perms[...,2]].btagCSVV2).to_numpy()\n",
    "    features[:,15] = ak.flatten(jets[perms[...,3]].btagCSVV2).to_numpy()\n",
    "    \n",
    "    # qgl of every jet\n",
    "    features[:,16] = ak.flatten(jets[perms[...,0]].qgl).to_numpy()\n",
    "    features[:,17] = ak.flatten(jets[perms[...,1]].qgl).to_numpy()\n",
    "    features[:,18] = ak.flatten(jets[perms[...,2]].qgl).to_numpy()\n",
    "    features[:,19] = ak.flatten(jets[perms[...,3]].qgl).to_numpy()\n",
    "    \n",
    "    #### calculate combination-level labels ####\n",
    "    permutation_labels = np.array(labels_dict[4])\n",
    "    \n",
    "    # which combination does the truth label correspond to?\n",
    "    which_combination = np.zeros(len(jets), dtype=int)\n",
    "    # no correct matches\n",
    "    which_anti_combination = np.zeros(labels.shape[0], dtype=int)\n",
    "    for i in range(12):\n",
    "        which_combination[(labels==permutation_labels[i,:]).all(1)] = i\n",
    "        which_anti_combination[np.invert((labels==permutation_labels[i,:]).any(1))] = i\n",
    "\n",
    "    # convert to combination-level truth label (-1, 0 or 1)\n",
    "    which_combination = list(zip(range(len(jets),), which_combination))\n",
    "    which_anti_combination = list(zip(range(labels.shape[0],), which_anti_combination))\n",
    "    \n",
    "    truth_labels = -1*np.ones((len(jets),12))\n",
    "    for i,tpl in enumerate(which_combination):\n",
    "        truth_labels[tpl]=1\n",
    "    for i,tpl in enumerate(which_anti_combination):\n",
    "        truth_labels[tpl]=0\n",
    "        \n",
    "        \n",
    "    #### flatten to combinations (easy to unflatten since each event always has 12 combinations) ####\n",
    "    labels = truth_labels.reshape((truth_labels.shape[0]*truth_labels.shape[1],1))\n",
    "    \n",
    "    return features, labels, which_combination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1112746b-9802-4895-88a6-707b62732fb7",
   "metadata": {},
   "source": [
    "### Defining a `coffea` Processor\n",
    "\n",
    "The processor returns the training features and labels we will use in our BDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a311fee4-b2d1-4d26-b076-047754aa45bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to create column accumulator from list\n",
    "def col_accumulator(a):\n",
    "    return processor.column_accumulator(np.array(a))\n",
    "\n",
    "processor_base = processor.ProcessorABC\n",
    "class JetClassifier(processor_base):\n",
    "    def __init__(self, permutations_dict, labels_dict):\n",
    "        super().__init__()\n",
    "        self.permutations_dict = permutations_dict\n",
    "        self.labels_dict = labels_dict\n",
    "    \n",
    "    def process(self, events):\n",
    "        \n",
    "        process = events.metadata[\"process\"]  # \"ttbar\" etc.\n",
    "        variation = events.metadata[\"variation\"]  # \"nominal\" etc.\n",
    "        \n",
    "        # normalization for MC\n",
    "        x_sec = events.metadata[\"xsec\"]\n",
    "        nevts_total = events.metadata[\"nevts\"]\n",
    "        lumi = 3378 # /pb\n",
    "        xsec_weight = x_sec * lumi / nevts_total\n",
    "            \n",
    "        events[\"pt_nominal\"] = 1.0\n",
    "        pt_variations = [\"pt_nominal\"] if variation == \"nominal\" else [\"pt_nominal\"]\n",
    "        for pt_var in pt_variations:\n",
    "            \n",
    "            # filter electrons, muons, and jets\n",
    "            selected_electrons = events.Electron[(events.Electron.pt > 30) & (np.abs(events.Electron.eta)<2.1) & \n",
    "                                                 (events.Electron.cutBased==4) & (events.Electron.sip3d < 4)]\n",
    "            selected_muons = events.Muon[(events.Muon.pt > 30) & (np.abs(events.Muon.eta)<2.1) & (events.Muon.tightId) & \n",
    "                                         (events.Muon.sip3d < 4) & (events.Muon.pfRelIso04_all < 0.15)]\n",
    "            jet_filter = (events.Jet.pt > 30) & (np.abs(events.Jet.eta) < 2.4) & (events.Jet.isTightLeptonVeto)\n",
    "            selected_jets = events.Jet[jet_filter]\n",
    "            selected_genpart = events.GenPart\n",
    "            even = (events.event%2==0)\n",
    "            \n",
    "            # single lepton requirement\n",
    "            event_filters = ((ak.count(selected_electrons.pt, axis=1) + ak.count(selected_muons.pt, axis=1)) == 1)\n",
    "            # require at least 4 jets\n",
    "            event_filters = event_filters & (ak.count(selected_jets.pt, axis=1) >= 4)\n",
    "            # require at least one jet above B_TAG_THRESHOLD\n",
    "            B_TAG_THRESHOLD = 0.5\n",
    "            event_filters = event_filters & (ak.sum(selected_jets.btagCSVV2 >= B_TAG_THRESHOLD, axis=1) >= 1)\n",
    "            \n",
    "            # apply event filters\n",
    "            selected_events = events[event_filters]\n",
    "            selected_electrons = selected_electrons[event_filters]\n",
    "            selected_muons = selected_muons[event_filters]\n",
    "            selected_jets = selected_jets[event_filters]\n",
    "            selected_genpart = selected_genpart[event_filters]\n",
    "            even = even[event_filters]\n",
    "            \n",
    "            ### only consider 4j2b region\n",
    "            region_filter = ak.sum(selected_jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 2 # at least two b-tagged jets\n",
    "            selected_jets_region = selected_jets[region_filter][:,:4] # only keep top 4 jets\n",
    "            selected_electrons_region = selected_electrons[region_filter]\n",
    "            selected_muons_region = selected_muons[region_filter]\n",
    "            selected_genpart_region = selected_genpart[region_filter]\n",
    "            even = even[region_filter]\n",
    "            \n",
    "            # filter events and calculate labels\n",
    "            jets, electrons, muons, labels, even = training_filter(selected_jets_region, \n",
    "                                                                   selected_electrons_region, \n",
    "                                                                   selected_muons_region, \n",
    "                                                                   selected_genpart_region,\n",
    "                                                                   even)\n",
    "            \n",
    "            \n",
    "            # calculate mbjj\n",
    "            # reconstruct hadronic top as bjj system with largest pT\n",
    "            # the jet energy scale / resolution effect is not propagated to this observable at the moment\n",
    "            trijet = ak.combinations(jets, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidates\n",
    "            trijet_labels = ak.combinations(labels, 3, fields=[\"j1\", \"j2\", \"j3\"])\n",
    "            trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # calculate four-momentum of tri-jet system\n",
    "            trijet[\"label\"] = trijet_labels.j1 + trijet_labels.j2 + trijet_labels.j3\n",
    "            trijet[\"max_btag\"] = np.maximum(trijet.j1.btagCSVV2, np.maximum(trijet.j2.btagCSVV2, trijet.j3.btagCSVV2))\n",
    "            trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # at least one-btag in trijet candidates\n",
    "            # pick trijet candidate with largest pT and calculate mass of system\n",
    "            trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "            trijet_label = trijet[\"label\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)]\n",
    "            observable = ak.flatten(trijet_mass)\n",
    "            trijet_label = ak.flatten(trijet_label)\n",
    "            \n",
    "            # calculate features and labels\n",
    "            features, labels, which_combination = get_training_set(jets, electrons, muons, labels,\n",
    "                                                                   self.permutations_dict, self.labels_dict)\n",
    "    \n",
    "            \n",
    "        output = {\"nevents\": {events.metadata[\"dataset\"]: len(events)},\n",
    "                  \"features\": col_accumulator(features.tolist()),\n",
    "                  \"labels\": col_accumulator(labels.tolist()),\n",
    "                  \"observable\": col_accumulator(observable.tolist()),\n",
    "                  \"even\": col_accumulator(even.tolist()),\n",
    "                  \"trijet_label\": col_accumulator(trijet_label.tolist()),}\n",
    "            \n",
    "        return output\n",
    "        \n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4c24f0-b45f-4c5e-a0ce-dc6afd960e74",
   "metadata": {},
   "source": [
    "### \"Fileset\" construction and metadata\n",
    "\n",
    "Here, we gather all the required information about the files we want to process: paths to the files and asociated metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45fc2b2-b966-4d74-ac33-342490d900b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fileset = utils.construct_fileset(N_FILES_MAX_PER_SAMPLE, \n",
    "                                  use_xcache=False)\n",
    "\n",
    "# get rid of everything except ttbar__nominal for training purposes\n",
    "fileset_keys = list(fileset.keys())\n",
    "for key in fileset_keys:\n",
    "    if key!=\"ttbar__nominal\":\n",
    "        fileset.pop(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b15da9-2f6d-4fa2-8c67-6e924360b04e",
   "metadata": {},
   "source": [
    "### Execute the data delivery pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d4acb1-816b-408a-8a91-e5ebbd99e340",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "NanoAODSchema.warn_missing_crossrefs = False\n",
    "\n",
    "if USE_DASK_PROCESSING:\n",
    "    executor = processor.DaskExecutor(client=utils.get_client(AF))\n",
    "else:\n",
    "    executor = processor.FuturesExecutor(workers=NUM_CORES)\n",
    "    \n",
    "run = processor.Runner(executor=executor, schema=NanoAODSchema, savemetrics=True, metadata_cache={}, \n",
    "                       chunksize=CHUNKSIZE)\n",
    "\n",
    "# preprocess\n",
    "filemeta = run.preprocess(fileset, treename=\"Events\")\n",
    "\n",
    "# process\n",
    "output, metrics = run(fileset, \n",
    "                      \"Events\", \n",
    "                      processor_instance = JetClassifier(permutations_dict, labels_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3085d626-09dc-4548-82cd-92793aec1237",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grab features and labels and convert to np array\n",
    "features = output['features'].value\n",
    "labels = output['labels'].value\n",
    "even = output['even'].value\n",
    "observable = output['observable'].value\n",
    "\n",
    "labels = labels.reshape((len(labels),))\n",
    "even = np.repeat(even, 12) # twelve permutations for each event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0859a175-ace2-4ce8-9934-92cbdda30028",
   "metadata": {},
   "source": [
    "The key for the labeling scheme is as follows\n",
    "\n",
    "* 1: all jet assignments are correct\n",
    "* 0: some jet assignments are correct (one or two are correct, others are incorrect)\n",
    "* -1: all jet assignments are incorrect\n",
    "\n",
    "There are twelve combinations for each event, so each event will have 1 correct combination, 2 completely incorrect combinations, and 9 partially correct combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d532f2-ef1c-4ea1-922f-dd1259bd91a9",
   "metadata": {},
   "source": [
    "# Histograms of Training Variables\n",
    "To vizualize the separation power of the different variables, histograms are created for each of the three labels. Only `all_correct` and `none_correct` are used for training purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5523c438-d438-4acb-9c49-19a4272909ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# separate by label for plotting\n",
    "all_correct = features[labels==1,:]\n",
    "some_correct = features[labels==-1,:]\n",
    "none_correct = features[labels==0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f52735-1a56-4346-8040-ef7db1037c10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### delta R histogram ####\n",
    "\n",
    "# binning\n",
    "deltar_low = 0.0\n",
    "deltar_high = 8.0\n",
    "deltar_numbins = 100\n",
    "legend_list = [\"All Matches Correct\", \"Some Matches Correct\", \"No Matches Correct\"]\n",
    "\n",
    "# define histogram\n",
    "h = hist.Hist(\n",
    "    hist.axis.Regular(deltar_numbins, deltar_low, deltar_high, name=\"deltar\", label=\"$\\Delta R$\", flow=False),\n",
    "    hist.axis.StrCategory(legend_list, name=\"truthlabel\", label=\"Truth Label\"),\n",
    "    hist.axis.StrCategory([\"toplep_lepton\",\"W_W\",\"tophad_W\"], name=\"category\", label=\"Category\"),\n",
    ")\n",
    "\n",
    "# fill histogram\n",
    "h.fill(deltar = all_correct[:,0], category=\"toplep_lepton\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(deltar = some_correct[:,0], category=\"toplep_lepton\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(deltar = none_correct[:,0], category=\"toplep_lepton\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(deltar = all_correct[:,1], category=\"W_W\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(deltar = some_correct[:,1], category=\"W_W\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(deltar = none_correct[:,1], category=\"W_W\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(deltar = all_correct[:,2], category=\"tophad_W\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(deltar = some_correct[:,2], category=\"tophad_W\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(deltar = none_correct[:,2], category=\"tophad_W\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(deltar = all_correct[:,3], category=\"tophad_W\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(deltar = some_correct[:,3], category=\"tophad_W\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(deltar = none_correct[:,3], category=\"tophad_W\", truthlabel=\"No Matches Correct\")\n",
    "\n",
    "# make plots\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[0j::hist.rebin(2), :, \"toplep_lepton\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list)\n",
    "ax.set_title(\"$\\Delta R$ between $top_{lepton}$ jet and lepton\")\n",
    "fig.show()\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[0j::hist.rebin(2), :, \"W_W\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list)\n",
    "ax.set_title(\"$\\Delta R$ between the two $W$ jets\")\n",
    "fig.show()\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[0j::hist.rebin(2), :, \"tophad_W\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list)\n",
    "ax.set_title(\"$\\Delta R$ between $W$ jet and $top_{hadron}$ jet\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb40ada0-8acd-4882-b489-49bf3eed16e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### mass histogram ####\n",
    "\n",
    "# binning\n",
    "combinedmass_low = 0.0\n",
    "combinedmass_high = 1500.0\n",
    "combinedmass_numbins = 200\n",
    "legend_list = [\"All Matches Correct\", \"Some Matches Correct\", \"No Matches Correct\"]\n",
    "\n",
    "# define histogram\n",
    "h = hist.Hist(\n",
    "    hist.axis.Regular(combinedmass_numbins, combinedmass_low, combinedmass_high, \n",
    "                      name=\"combinedmass\", label=\"Combined Mass [GeV]\", flow=False),\n",
    "    hist.axis.StrCategory(legend_list, name=\"truthlabel\", label=\"Truth Label\"),\n",
    "    hist.axis.StrCategory([\"toplep_lepton\",\"W_W\",\"tophad_W_W\"], name=\"category\", label=\"Category\"),\n",
    ")\n",
    "\n",
    "# fill histogram\n",
    "h.fill(combinedmass = all_correct[:,4], category=\"toplep_lepton\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(combinedmass = some_correct[:,4], category=\"toplep_lepton\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(combinedmass = none_correct[:,4], category=\"toplep_lepton\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(combinedmass = all_correct[:,5], category=\"W_W\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(combinedmass = some_correct[:,5], category=\"W_W\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(combinedmass = none_correct[:,5], category=\"W_W\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(combinedmass = all_correct[:,6], category=\"tophad_W_W\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(combinedmass = some_correct[:,6], category=\"tophad_W_W\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(combinedmass = none_correct[:,6], category=\"tophad_W_W\", truthlabel=\"No Matches Correct\")\n",
    "\n",
    "# make plots\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[:, :, \"toplep_lepton\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list[:-1])\n",
    "ax.set_title(\"Combined mass of $top_{lepton}$ jet and lepton\")\n",
    "ax.set_xlim([0,400])\n",
    "fig.show()\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[:, :, \"W_W\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list[:-1])\n",
    "ax.set_title(\"Combined mass of the two $W$ jets\")\n",
    "ax.set_xlim([0,400])\n",
    "fig.show()\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[:, :, \"tophad_W_W\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list)\n",
    "ax.set_title(\"Combined mass of $W$ jets and $top_{hadron}$ jet (Reconstructed Top Mass)\")\n",
    "ax.set_xlim([0,600])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0be88a-6812-41ee-9dbf-94703bb09938",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### combined pT histogram ####\n",
    "\n",
    "# binning\n",
    "combinedpt_low = 0.0\n",
    "combinedpt_high = 1000.0\n",
    "combinedpt_numbins = 200\n",
    "legend_list = [\"All Matches Correct\", \"Some Matches Correct\", \"No Matches Correct\"]\n",
    "\n",
    "# define histogram\n",
    "h = hist.Hist(\n",
    "    hist.axis.Regular(combinedpt_numbins, combinedpt_low, combinedpt_high, \n",
    "                      name=\"pt\", label=\"pT [GeV]\", flow=False),\n",
    "    hist.axis.StrCategory(legend_list, name=\"truthlabel\", label=\"Truth Label\"),\n",
    ")\n",
    "\n",
    "# fill histogram\n",
    "h.fill(pt = all_correct[:,7], truthlabel=\"All Matches Correct\")\n",
    "h.fill(pt = some_correct[:,7], truthlabel=\"Some Matches Correct\")\n",
    "h.fill(pt = none_correct[:,7], truthlabel=\"No Matches Correct\")\n",
    "\n",
    "# make plots\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h.plot(density=True, ax=ax)\n",
    "ax.legend(legend_list)\n",
    "ax.set_title(\"Combined pT of W jets and top_hadron jet\")\n",
    "ax.set_xlim([0,600])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f25b60-a955-4534-a777-9c3c734382d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### pT histogram ####\n",
    "\n",
    "# binning\n",
    "pt_low = 25.0\n",
    "pt_high = 300.0\n",
    "pt_numbins = 100\n",
    "legend_list = [\"All Matches Correct\", \"Some Matches Correct\", \"No Matches Correct\"]\n",
    "\n",
    "# define histogram\n",
    "h = hist.Hist(\n",
    "    hist.axis.Regular(pt_numbins, pt_low, pt_high, \n",
    "                      name=\"jetpt\", label=\"Jet $p_T$ [GeV]\", flow=False),\n",
    "    hist.axis.StrCategory(legend_list, name=\"truthlabel\", label=\"Truth Label\"),\n",
    "    hist.axis.StrCategory([\"W\",\"toplep\",\"tophad\"], name=\"category\", label=\"Category\"),\n",
    ")\n",
    "\n",
    "# fill histogram\n",
    "h.fill(jetpt = all_correct[:,8], category=\"W\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(jetpt = some_correct[:,8], category=\"W\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(jetpt = none_correct[:,8], category=\"W\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(jetpt = all_correct[:,9], category=\"W\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(jetpt = some_correct[:,9], category=\"W\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(jetpt = none_correct[:,9], category=\"W\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(jetpt = all_correct[:,10], category=\"tophad\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(jetpt = some_correct[:,10], category=\"tophad\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(jetpt = none_correct[:,10], category=\"tophad\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(jetpt = all_correct[:,11], category=\"toplep\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(jetpt = some_correct[:,11], category=\"toplep\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(jetpt = none_correct[:,11], category=\"toplep\", truthlabel=\"No Matches Correct\")\n",
    "\n",
    "# make plots\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[:, :, \"W\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list)\n",
    "ax.set_title(\"W Jet $p_T$\")\n",
    "ax.set_xlim([25,300])\n",
    "fig.show()\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[:, :, \"tophad\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list)\n",
    "ax.set_title(\"top_hadron Jet $p_T$\")\n",
    "ax.set_xlim([25,300])\n",
    "fig.show()\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[:, :, \"toplep\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list)\n",
    "ax.set_title(\"top_lepton Jet $p_T$\")\n",
    "ax.set_xlim([25,200])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9332644c-fce7-4172-ba60-0fc7aa70aa6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### btag histogram ####\n",
    "\n",
    "# binning\n",
    "btag_low = 0.0\n",
    "btag_high = 1.0\n",
    "btag_numbins = 50\n",
    "legend_list = [\"All Matches Correct\", \"Some Matches Correct\", \"No Matches Correct\"]\n",
    "\n",
    "# define histogram\n",
    "h = hist.Hist(\n",
    "    hist.axis.Regular(btag_numbins, btag_low, btag_high, \n",
    "                      name=\"btag\", label=\"Jet btag\", flow=False),\n",
    "    hist.axis.StrCategory(legend_list, name=\"truthlabel\", label=\"Truth Label\"),\n",
    "    hist.axis.StrCategory([\"W\",\"toplep\",\"tophad\"], name=\"category\", label=\"Category\"),\n",
    ")\n",
    "\n",
    "# fill histogram\n",
    "h.fill(btag = all_correct[:,12], category=\"W\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(btag = some_correct[:,12], category=\"W\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(btag = none_correct[:,12], category=\"W\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(btag = all_correct[:,13], category=\"W\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(btag = some_correct[:,13], category=\"W\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(btag = none_correct[:,13], category=\"W\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(btag = all_correct[:,14], category=\"tophad\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(btag = some_correct[:,14], category=\"tophad\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(btag = none_correct[:,14], category=\"tophad\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(btag = all_correct[:,15], category=\"toplep\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(btag = some_correct[:,15], category=\"toplep\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(btag = none_correct[:,15], category=\"toplep\", truthlabel=\"No Matches Correct\")\n",
    "\n",
    "# make plots\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[:, :, \"W\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list)\n",
    "ax.set_title(\"W Jet btag\")\n",
    "fig.show()\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[:, :, \"tophad\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list)\n",
    "ax.set_title(\"top_hadron Jet btag\")\n",
    "fig.show()\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[:, :, \"toplep\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list)\n",
    "ax.set_title(\"top_lepton Jet btag\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cf5e42-0bf8-426b-a45e-0161a1bc8772",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### qgl histogram ####\n",
    "\n",
    "# binning\n",
    "qgl_low = -1.0\n",
    "qgl_high = 1.0\n",
    "qgl_numbins = 50\n",
    "legend_list = [\"All Matches Correct\", \"Some Matches Correct\", \"No Matches Correct\"]\n",
    "\n",
    "# define histogram\n",
    "h = hist.Hist(\n",
    "    hist.axis.Regular(qgl_numbins, qgl_low, qgl_high, \n",
    "                      name=\"qgl\", label=\"Jet qgl\", flow=False),\n",
    "    hist.axis.StrCategory(legend_list, name=\"truthlabel\", label=\"Truth Label\"),\n",
    "    hist.axis.StrCategory([\"W\",\"toplep\",\"tophad\"], name=\"category\", label=\"Category\"),\n",
    ")\n",
    "\n",
    "# fill histogram\n",
    "h.fill(qgl = all_correct[:,16], category=\"W\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(qgl = some_correct[:,16], category=\"W\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(qgl = none_correct[:,16], category=\"W\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(qgl = all_correct[:,17], category=\"W\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(qgl = some_correct[:,17], category=\"W\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(qgl = none_correct[:,17], category=\"W\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(qgl = all_correct[:,18], category=\"tophad\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(qgl = some_correct[:,18], category=\"tophad\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(qgl = none_correct[:,18], category=\"tophad\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(qgl = all_correct[:,19], category=\"toplep\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(qgl = some_correct[:,19], category=\"toplep\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(qgl = none_correct[:,19], category=\"toplep\", truthlabel=\"No Matches Correct\")\n",
    "\n",
    "# make plots\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[:, :, \"W\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list)\n",
    "ax.set_title(\"W Jet qgl\")\n",
    "fig.show()\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[:, :, \"tophad\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list)\n",
    "ax.set_title(\"top_hadron Jet qgl\")\n",
    "fig.show()\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[:, :, \"toplep\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list)\n",
    "ax.set_title(\"top_lepton Jet qgl\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a534fc84-a8c4-4ac0-8b18-14b12de6ed8e",
   "metadata": {},
   "source": [
    "# Model Optimization\n",
    "\n",
    "The model used here is `xgboost`'s gradient-boosted decision tree (`XGBClassifier`). Hyperparameter optimization is performed using random selection from a sample space of hyperparameters then testing model fits in a parallelized manner using `dask`. Optional `mlflow` logging is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590a526b-6848-472d-a299-0c768caafaff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grab features and labels and convert to np array\n",
    "features = output['features'].value\n",
    "labels = output['labels'].value\n",
    "labels[labels==-1]=0 # partially correct = wrong\n",
    "even = output['even'].value\n",
    "\n",
    "features = features.reshape((int(features.shape[0]/12),12,20))\n",
    "labels = labels.reshape((int(labels.shape[0]/12),12))\n",
    "\n",
    "shuffle_indices = np.array(range(features.shape[0])).astype(int)\n",
    "np.random.shuffle(shuffle_indices)\n",
    "\n",
    "features = features[shuffle_indices]\n",
    "labels = labels[shuffle_indices]\n",
    "which_combination = np.argmax(labels,axis=-1)\n",
    "even = even[shuffle_indices]\n",
    "\n",
    "features_even = features[even]\n",
    "features_even = features_even.reshape((int(12*features_even.shape[0]),20))\n",
    "labels_even = labels[even]\n",
    "labels_even = labels_even.reshape((int(12*labels_even.shape[0]),))\n",
    "which_combination_even = which_combination[even]\n",
    "\n",
    "features_odd = features[np.invert(even)]\n",
    "features_odd = features_odd.reshape((int(12*features_odd.shape[0]),20))\n",
    "labels_odd = labels[np.invert(even)]\n",
    "labels_odd = labels_odd.reshape((int(12*labels_odd.shape[0]),))\n",
    "which_combination_odd = which_combination[np.invert(even)]\n",
    "\n",
    "print(\"features_even.shape = \", features_even.shape)\n",
    "print(\"features_odd.shape = \", features_odd.shape)\n",
    "\n",
    "N_EVENTS_TRAIN = min(min(int(features_odd.shape[0]/12), N_EVENTS_TRAIN), int(features_even.shape[0]/12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7e2759-5528-4329-915e-4784d87e04bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up trials\n",
    "if USE_MLFLOW:\n",
    "    \n",
    "    os.environ['MLFLOW_TRACKING_TOKEN'] = \"\" # enter token here\n",
    "    os.environ['MLFLOW_TRACKING_URI'] = \"https://mlflow-demo.software-dev.ncsa.illinois.edu\"\n",
    "    \n",
    "    mlflow.set_tracking_uri('https://mlflow-demo.software-dev.ncsa.illinois.edu') \n",
    "    mlflow.set_experiment(\"optimize-reconstruction-bdt-00\") # this will create the experiment if it does not yet exist\n",
    "\n",
    "    # grab experiment\n",
    "    current_experiment=dict(mlflow.get_experiment_by_name(\"optimize-reconstruction-bdt-00\"))\n",
    "    experiment_id=current_experiment['experiment_id']\n",
    "    print(\"experiment_id = \", experiment_id)\n",
    "\n",
    "    # create runs ahead of time (avoids conflicts when parallelizing mlflow logging)\n",
    "    run_id_list=[]\n",
    "    for n in range(N_TRIALS*2):\n",
    "        run = MlflowClient().create_run(experiment_id=experiment_id, run_name=f\"run-{n}\")\n",
    "        run_id_list.append(run.info.run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfef7c10-42bb-43ff-89f5-541f95b3860a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampler = ParameterSampler({'max_depth': np.arange(1,81,10,dtype=int), \n",
    "                            'n_estimators': np.arange(1,501,50,dtype=int), \n",
    "                            'learning_rate': np.linspace(0.01, 1, 10),\n",
    "                            'min_child_weight': np.logspace(-1, 3, 20), \n",
    "                            'reg_lambda': [0, 0.25, 0.5, 0.75, 1], \n",
    "                            'reg_alpha': [0, 0.25, 0.5, 0.75, 1],\n",
    "                            'gamma': np.logspace(-4, 1, 20),\n",
    "                            'tree_method': [\"hist\"],}, \n",
    "                            n_iter = N_TRIALS, \n",
    "                            random_state=2) \n",
    "\n",
    "samples_even = list(sampler)\n",
    "samples_odd = list(sampler)\n",
    "\n",
    "# add additional info to each trial\n",
    "for i in range(N_TRIALS):\n",
    "    samples_even[i]['trial_num'] = i\n",
    "    samples_even[i]['parity'] = 'even' # categorizes this trial as for even event numbers\n",
    "    \n",
    "    samples_odd[i]['trial_num'] = i\n",
    "    samples_odd[i]['parity'] = 'odd' # categorizes this trial as for odd event numbers\n",
    "    \n",
    "    if USE_MLFLOW: \n",
    "        samples_even[i]['run_id'] = run_id_list[i]\n",
    "        samples_odd[i]['run_id'] = run_id_list[i+N_TRIALS]\n",
    "    \n",
    "print(\"Example of Trial Parameters: \")\n",
    "samples_even[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff522456-6371-48dd-b6c5-9820e921ea44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if USE_MLFLOW:\n",
    "    # set mlflowclient\n",
    "    mlflowclient = MlflowClient()\n",
    "else: \n",
    "    mlflowclient = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22b93ab-f976-4b08-9a33-1a3c58e876b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def modified_cross_validation(model, \n",
    "                              features, labels, \n",
    "                              evaluation_matrix, n_folds=2):\n",
    "            \n",
    "    features = features.reshape((int(features.shape[0]/12),12,20))\n",
    "    labels = labels.reshape((int(labels.shape[0]/12),12))\n",
    "    which_combination = np.argmax(labels, axis=-1)\n",
    "        \n",
    "    shuffle_ind = np.array(range(features.shape[0])).astype(int)\n",
    "    np.random.shuffle(shuffle_ind)\n",
    "    splits = np.array_split(shuffle_ind, n_folds)\n",
    "    \n",
    "    test_accuracy = np.zeros(n_folds)\n",
    "    test_precision = np.zeros(n_folds)\n",
    "    test_recall = np.zeros(n_folds)\n",
    "    test_f1 = np.zeros(n_folds)\n",
    "    test_roc_auc = np.zeros(n_folds)\n",
    "    test_jet_score = np.zeros(n_folds)\n",
    "    \n",
    "    train_accuracy = np.zeros(n_folds)\n",
    "    train_precision = np.zeros(n_folds)\n",
    "    train_recall = np.zeros(n_folds)\n",
    "    train_f1 = np.zeros(n_folds)\n",
    "    train_roc_auc = np.zeros(n_folds)\n",
    "    train_jet_score = np.zeros(n_folds)\n",
    "    \n",
    "    for n in range(n_folds):\n",
    "        \n",
    "        features_test = features[splits[n]]\n",
    "        features_test = features_test.reshape((12*features_test.shape[0],20))\n",
    "        labels_test = labels[splits[n]]\n",
    "        labels_test = labels_test.reshape((12*labels_test.shape[0],))\n",
    "        which_combination_test = which_combination[splits[n]]\n",
    "        \n",
    "        train_ind = np.concatenate([splits[i] for i in range(n_folds) if not i==n])\n",
    "        \n",
    "        features_train = features[train_ind]\n",
    "        features_train = features_train.reshape((12*features_train.shape[0],20))\n",
    "        labels_train = labels[train_ind]\n",
    "        labels_train = labels_train.reshape((12*labels_train.shape[0],))\n",
    "        which_combination_train = which_combination[train_ind]\n",
    "                \n",
    "        model.fit(features_train, labels_train)\n",
    "        \n",
    "        test_predictions = model.predict(features_test)\n",
    "        train_predictions = model.predict(features_train)\n",
    "        \n",
    "        test_accuracy[n] = accuracy_score(labels_test, test_predictions)\n",
    "        test_precision[n] = precision_score(labels_test, test_predictions)\n",
    "        test_recall[n] = recall_score(labels_test, test_predictions)\n",
    "        test_f1[n] = f1_score(labels_test, test_predictions)\n",
    "        test_roc_auc[n] = roc_auc_score(labels_test, test_predictions)\n",
    "        \n",
    "        train_accuracy[n] = accuracy_score(labels_train, train_predictions)\n",
    "        train_precision[n] = precision_score(labels_train, train_predictions)\n",
    "        train_recall[n] = recall_score(labels_train, train_predictions)\n",
    "        train_f1[n] = f1_score(labels_train, train_predictions)\n",
    "        train_roc_auc[n] = roc_auc_score(labels_train, train_predictions)\n",
    "        \n",
    "        \n",
    "        test_predictions_prob = model.predict_proba(features_test)[:,1]\n",
    "        train_predictions_prob = model.predict_proba(features_train)[:,1]\n",
    "        test_predictions_prob = test_predictions_prob.reshape((int(test_predictions_prob.shape[0]/12),12))\n",
    "        train_predictions_prob = train_predictions_prob.reshape((int(train_predictions_prob.shape[0]/12),12))\n",
    "        \n",
    "        train_predicted_combination = np.argmax(train_predictions_prob,axis=1)\n",
    "        scores = np.zeros(len(which_combination_train))\n",
    "        zipped = list(zip(which_combination_train.tolist(), train_predicted_combination.tolist()))\n",
    "        for i in range(len(which_combination_train)):\n",
    "            scores[i] = evaluation_matrix[zipped[i]]\n",
    "        train_jet_score[n] = sum(scores)/len(scores)\n",
    "        \n",
    "        test_predicted_combination = np.argmax(test_predictions_prob,axis=1)\n",
    "        scores = np.zeros(len(which_combination_test))\n",
    "        zipped = list(zip(which_combination_test.tolist(), test_predicted_combination.tolist()))\n",
    "        for i in range(len(which_combination_test)):\n",
    "            scores[i] = evaluation_matrix[zipped[i]]\n",
    "        test_jet_score[n] = sum(scores)/len(scores)\n",
    "        \n",
    "    \n",
    "    return {\"test_accuracy\": test_accuracy, \n",
    "            \"test_precision\": test_precision, \n",
    "            \"test_recall\": test_recall,\n",
    "            \"test_f1\": test_f1,\n",
    "            \"test_roc_auc\": test_roc_auc,\n",
    "            \"test_jet_score\": test_jet_score,\n",
    "            \"train_accuracy\": train_accuracy, \n",
    "            \"train_precision\": train_precision, \n",
    "            \"train_recall\": train_recall,\n",
    "            \"train_f1\": train_f1,\n",
    "            \"train_roc_auc\": train_roc_auc, \n",
    "            \"train_jet_score\": train_jet_score,\n",
    "            \"model\": model\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20d3607-b3f6-4070-8f6f-bc7257d7236e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fit_model(params, \n",
    "              features, \n",
    "              labels, \n",
    "              evaluation_matrix,\n",
    "              n_folds,\n",
    "              mlflowclient=None,\n",
    "              use_mlflow=False,\n",
    "              log_models=False,\n",
    "              verbose=False): \n",
    "                            \n",
    "    if use_mlflow:\n",
    "        \n",
    "        run_id = params[\"run_id\"]\n",
    "        \n",
    "        if verbose: print(\"run_id = \", run_id)\n",
    "        \n",
    "        for param_name, value in params.items():\n",
    "            mlflowclient.log_param(run_id, param_name, value)\n",
    "            \n",
    "            if verbose: print(f\"logged param: {param_name} = {value}\")\n",
    "            \n",
    "    # remove parameters that are not used for XGBClassifier\n",
    "    params_copy = params.copy()\n",
    "    params_copy.pop(\"trial_num\")\n",
    "    params_copy.pop(\"parity\")\n",
    "    if use_mlflow: params_copy.pop(\"run_id\")\n",
    "    \n",
    "    # initialize model with current trial paramters\n",
    "    model = XGBClassifier(random_state=5, \n",
    "                          nthread=-1,\n",
    "                          **params_copy) \n",
    "\n",
    "    # perform n-fold cross-validation\n",
    "    result = modified_cross_validation(model, features, labels,\n",
    "                                      evaluation_matrix, n_folds=n_folds)\n",
    "    \n",
    "    if use_mlflow:\n",
    "        for metric, value in result.items():\n",
    "            if not metric==\"model\":\n",
    "                mlflowclient.log_metric(run_id, metric, np.average(value))\n",
    "                if verbose: print(f\"logged metric: {metric} = {np.average(value)}\")\n",
    "\n",
    "        # manually end run\n",
    "        mlflowclient.set_terminated(run_id)\n",
    "        \n",
    "        if log_models:\n",
    "            signature = infer_signature(features, result[\"model\"].predict(features))\n",
    "            with mlflow.start_run(run_id=run_id, nested=True) as run:\n",
    "                mlflow.xgboost.log_model(result[\"model\"], \"model\", signature=signature)\n",
    "            result.pop(\"model\")\n",
    "                \n",
    "    if not log_models:\n",
    "        return {\"score\": np.average(result[\"test_jet_score\"]),\n",
    "                \"full_result\": result}\n",
    "    return {\"score\": np.average(result[\"test_jet_score\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1eac61-82ca-49b2-94d8-5817f362da62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to provide necessary environment variables to workers\n",
    "def initialize_mlflow(): \n",
    "    \n",
    "    os.environ['MLFLOW_TRACKING_TOKEN'] = \"\" # enter token here\n",
    "    os.environ['MLFLOW_TRACKING_URI'] = \"https://mlflow-demo.software-dev.ncsa.illinois.edu\"\n",
    "    \n",
    "    mlflow.set_tracking_uri('https://mlflow-demo.software-dev.ncsa.illinois.edu') \n",
    "    mlflow.set_experiment(\"optimize-reconstruction-bdt-00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c138307-4d72-49cf-93bf-2e92fb66a3a6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if USE_DASK_ML:\n",
    "    start_time = time.time() \n",
    "    \n",
    "    client = utils.get_client()\n",
    "    if USE_MLFLOW:\n",
    "        client.run(initialize_mlflow)\n",
    "    \n",
    "    futures = client.map(fit_model,\n",
    "                         samples_even, \n",
    "                         features=features_even[:N_EVENTS_TRAIN*12], \n",
    "                         labels=labels_even[:N_EVENTS_TRAIN*12],\n",
    "                         evaluation_matrix=evaluation_matrix,\n",
    "                         n_folds=N_FOLD,\n",
    "                         mlflowclient=mlflowclient,\n",
    "                         use_mlflow=USE_MLFLOW,\n",
    "                         log_models=MODEL_LOGGING) \n",
    "\n",
    "    res = client.gather(futures)\n",
    "    time_elapsed = time.time() - start_time\n",
    "    \n",
    "else:\n",
    "    start_time = time.time() \n",
    "    res = []\n",
    "    for i in range(len(samples_even)):\n",
    "        print(\"_____________________________________________________________\")\n",
    "        print(i)\n",
    "        print(samples_even[i])\n",
    "        res.append(fit_model(samples_even[i], \n",
    "                             features=features_even[:N_EVENTS_TRAIN*12],\n",
    "                             labels=labels_even[:N_EVENTS_TRAIN*12], \n",
    "                             evaluation_matrix=evaluation_matrix,\n",
    "                             n_folds=N_FOLD,\n",
    "                             mlflowclient=mlflowclient,\n",
    "                             use_mlflow=USE_MLFLOW,\n",
    "                             log_models=MODEL_LOGGING))\n",
    "        print(res[i])\n",
    "    time_elapsed = time.time() - start_time\n",
    "\n",
    "print(\"Hyperparameter optimization took time = \", time_elapsed)\n",
    "print()\n",
    "\n",
    "scores = [res[i][\"score\"] for i in range(len(res))]\n",
    "best_parameters_even = samples_even[np.argmax(scores)]\n",
    "print(\"best_parameters_even = \")\n",
    "best_parameters_even"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516438b2-57d2-4ff4-84c2-47a05eaf10ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if MODEL_LOGGING and USE_MLFLOW:\n",
    "    best_run_id = samples_even[np.argmax(scores)][\"run_id\"]\n",
    "    best_model_path = f'runs:/{best_run_id}/model'\n",
    "    best_model_even = mlflow.xgboost.load_model(best_model_path)\n",
    "    \n",
    "    # register best model in mlflow model repository\n",
    "    if MODEL_REGISTERING:\n",
    "        result = mlflow.register_model(best_model_path, \"reconstruction-bdt\")\n",
    "\n",
    "else:\n",
    "    best_model_even = res[np.argmax(scores)][\"full_result\"][\"model\"]\n",
    "    \n",
    "best_model_even.save_model(f\"models/model_{datetime.datetime.today().strftime('%y%m%d')}_even.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cc341b-4375-4c32-aff7-432a9fb445b7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if USE_DASK_ML:\n",
    "    start_time = time.time() \n",
    "    \n",
    "    client = utils.get_client()\n",
    "    client.run(initialize_mlflow)\n",
    "    \n",
    "    futures = client.map(fit_model,\n",
    "                         samples_odd, \n",
    "                         features=features_odd[:N_EVENTS_TRAIN*12], \n",
    "                         labels=labels_odd[:N_EVENTS_TRAIN*12],\n",
    "                         evaluation_matrix=evaluation_matrix,\n",
    "                         n_folds=N_FOLD,\n",
    "                         mlflowclient=mlflowclient,\n",
    "                         use_mlflow=USE_MLFLOW,\n",
    "                         log_models=MODEL_LOGGING) \n",
    "\n",
    "    res = client.gather(futures)\n",
    "    time_elapsed = time.time() - start_time\n",
    "    \n",
    "else:\n",
    "    start_time = time.time() \n",
    "    res = []\n",
    "    for i in range(len(samples_odd)):\n",
    "        print(\"_____________________________________________________________\")\n",
    "        print(i)\n",
    "        print(samples_odd[i])\n",
    "        res.append(fit_model(samples_odd[i], \n",
    "                             features=features_odd[:N_EVENTS_TRAIN*12],\n",
    "                             labels=labels_odd[:N_EVENTS_TRAIN*12], \n",
    "                             evaluation_matrix=evaluation_matrix,\n",
    "                             n_folds=N_FOLD,\n",
    "                             mlflowclient=mlflowclient,\n",
    "                             use_mlflow=USE_MLFLOW,\n",
    "                             log_models=MODEL_LOGGING))\n",
    "        print(res[i])\n",
    "    time_elapsed = time.time() - start_time\n",
    "\n",
    "print(\"Hyperparameter optimization took time = \", time_elapsed)\n",
    "print()\n",
    "\n",
    "scores = [res[i][\"score\"] for i in range(len(res))]\n",
    "best_parameters_odd = samples_odd[np.argmax(scores)]\n",
    "print(\"best_parameters_odd = \")\n",
    "best_parameters_odd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58bef97-e1f1-425c-b1f3-90a3d74ca32d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if MODEL_LOGGING and USE_MLFLOW:\n",
    "    best_run_id = samples_odd[np.argmax(scores)][\"run_id\"]\n",
    "    best_model_path = f'runs:/{best_run_id}/model'\n",
    "    best_model_odd = mlflow.xgboost.load_model(best_model_path)\n",
    "    \n",
    "    # register best model in mlflow model repository\n",
    "    if MODEL_REGISTERING:\n",
    "        result = mlflow.register_model(best_model_path, \"reconstruction-bdt\")\n",
    "\n",
    "else:\n",
    "    best_model_odd = res[np.argmax(scores)][\"full_result\"][\"model\"]\n",
    "    \n",
    "best_model_odd.save_model(f\"models/model_{datetime.datetime.today().strftime('%y%m%d')}_odd.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080b3617-cc4a-4598-b86b-95796f3ee71d",
   "metadata": {},
   "source": [
    "# Evaluation with Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c88c64-58df-4cdf-b8cd-c884e5a315d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generating Triton config file\n",
    "config_txt = utils.generate_triton_config(\"reconstruction_bdt_xgb\", \n",
    "                                          20, \n",
    "                                          predict_proba=True)\n",
    "print(config_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785a1301-0220-4ccc-8d38-a7b1edbafb0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir reconstruction_bdt_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db80b47-cb66-4eae-b5f0-d19e69d9f69a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f'reconstruction_bdt_xgb/config.pbtxt', 'w') as the_file:\n",
    "    the_file.write(config_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adfa5aa-6a53-43bc-966b-ef9db2252d89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir reconstruction_bdt_xgb/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3da7460-08b4-4594-bc32-6026f8142f18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model_even.save_model(\"reconstruction_bdt_xgb/1/xgboost.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47113ac3-3196-421f-8b4f-cd56393104ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir reconstruction_bdt_xgb/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1dfd67-3628-4731-93f0-13796229e25e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model_even.save_model(\"reconstruction_bdt_xgb/2/xgboost.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0ba766-a91b-4f1c-9761-ffc971eb4aa2",
   "metadata": {},
   "source": [
    "If you are using UNL open data, you can upload the model repository to the Triton server using `mc` in the command line:\n",
    "\n",
    "```\n",
    "mc alias set triton http://$BUCKET_HOST $AWS_ACCESS_KEY_ID $AWS_SECRET_ACCESS_KEY\n",
    "mc cp -r reconstruction_bdt_xgb triton/$BUCKET_NAME/reconstruction_bdt_xgb/\n",
    "```\n",
    "\n",
    "The server may need to be restarted in order to load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d730ac4-89cd-45d5-833c-5a47baf7c5df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove model directory after uploading to triton\n",
    "!rm -r reconstruction_bdt_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6dd51a",
   "metadata": {},
   "source": [
    "### Evaluation with Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf68c830-e234-4695-a5e1-1b62ec31ec31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "train_predicted = best_model_even.predict(features_even)\n",
    "train_predicted_prob = best_model_even.predict_proba(features_even)[:, 1]\n",
    "val_predicted = best_model_even.predict(features_odd)\n",
    "val_predicted_prob = best_model_even.predict_proba(features_odd)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9197c616-e84d-4818-82d8-06def49c82c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_accuracy = accuracy_score(labels_even, train_predicted).round(3)\n",
    "train_precision = precision_score(labels_even, train_predicted).round(3)\n",
    "train_recall = recall_score(labels_even, train_predicted).round(3)\n",
    "train_f1 = f1_score(labels_even, train_predicted).round(3)\n",
    "train_aucroc = roc_auc_score(labels_even, train_predicted_prob).round(3)\n",
    "print(\"Training Accuracy = \", train_accuracy)\n",
    "print(\"Training Precision = \", train_precision)\n",
    "print(\"Training Recall = \", train_recall)\n",
    "print(\"Training f1 = \", train_f1)\n",
    "print(\"Training AUC = \", train_aucroc)\n",
    "print()\n",
    "\n",
    "val_accuracy = accuracy_score(labels_odd, val_predicted).round(3)\n",
    "val_precision = precision_score(labels_odd, val_predicted).round(3)\n",
    "val_recall = recall_score(labels_odd, val_predicted).round(3)\n",
    "val_f1 = f1_score(labels_odd, val_predicted).round(3)\n",
    "val_aucroc = roc_auc_score(labels_odd, val_predicted_prob).round(3)\n",
    "print(\"Validation Accuracy = \", val_accuracy)\n",
    "print(\"Validation Precision = \", val_precision)\n",
    "print(\"Validation Recall = \", val_recall)\n",
    "print(\"Validation f1 = \", val_f1)\n",
    "print(\"Validation AUC = \", val_aucroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19afd15-1e6c-45b2-b2ba-35634e00ebf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_predicted_prob = val_predicted_prob.reshape((int(len(val_predicted_prob)/12),12))\n",
    "val_predicted_combination = np.argmax(val_predicted_prob,axis=1)\n",
    "    \n",
    "scores = np.zeros(len(which_combination_odd))\n",
    "zipped = list(zip(which_combination_odd.tolist(), val_predicted_combination.tolist()))\n",
    "for i in range(len(which_combination_odd)):\n",
    "    scores[i] = evaluation_matrix[zipped[i]]\n",
    "        \n",
    "score = sum(scores)/len(scores)\n",
    "print(\"Validation Jet Score = \", score)\n",
    "\n",
    "print(\"How many events are 100% correct: \", sum(scores==1)/len(scores), \", Random = \",sum(evaluation_matrix[0,:]==1)/12)\n",
    "print(\"How many events are 50% correct: \", sum(scores==0.5)/len(scores), \", Random = \",sum(evaluation_matrix[0,:]==0.5)/12)\n",
    "print(\"How many events are 25% correct: \", sum(scores==0.25)/len(scores), \", Random = \",sum(evaluation_matrix[0,:]==0.25)/12)\n",
    "print(\"How many events are 0% correct: \", sum(scores==0)/len(scores), \", Random = \",sum(evaluation_matrix[0,:]==0)/12)\n",
    "\n",
    "train_predicted_prob = train_predicted_prob.reshape((int(len(train_predicted_prob)/12),12))\n",
    "train_predicted_combination = np.argmax(train_predicted_prob,axis=1)\n",
    "    \n",
    "scores = np.zeros(len(which_combination_even))\n",
    "zipped = list(zip(which_combination_even.tolist(), train_predicted_combination.tolist()))\n",
    "for i in range(len(which_combination_even)):\n",
    "    scores[i] = evaluation_matrix[zipped[i]]\n",
    "        \n",
    "score = sum(scores)/len(scores)\n",
    "print(\"Training Jet Score = \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41dc77a-9751-4dfc-b2a8-6f063e06dee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make predictions\n",
    "train_predicted = best_model_odd.predict(features_odd)\n",
    "train_predicted_prob = best_model_odd.predict_proba(features_odd)[:, 1]\n",
    "val_predicted = best_model_odd.predict(features_even)\n",
    "val_predicted_prob = best_model_odd.predict_proba(features_even)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a05065-59c2-4c50-9c8e-71ec44ed3dfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_accuracy = accuracy_score(labels_odd, train_predicted).round(3)\n",
    "train_precision = precision_score(labels_odd, train_predicted).round(3)\n",
    "train_recall = recall_score(labels_odd, train_predicted).round(3)\n",
    "train_f1 = f1_score(labels_odd, train_predicted).round(3)\n",
    "train_aucroc = roc_auc_score(labels_odd, train_predicted_prob).round(3)\n",
    "print(\"Training Accuracy = \", train_accuracy)\n",
    "print(\"Training Precision = \", train_precision)\n",
    "print(\"Training Recall = \", train_recall)\n",
    "print(\"Training f1 = \", train_f1)\n",
    "print(\"Training AUC = \", train_aucroc)\n",
    "print()\n",
    "\n",
    "val_accuracy = accuracy_score(labels_even, val_predicted).round(3)\n",
    "val_precision = precision_score(labels_even, val_predicted).round(3)\n",
    "val_recall = recall_score(labels_even, val_predicted).round(3)\n",
    "val_f1 = f1_score(labels_even, val_predicted).round(3)\n",
    "val_aucroc = roc_auc_score(labels_even, val_predicted_prob).round(3)\n",
    "print(\"Validation Accuracy = \", val_accuracy)\n",
    "print(\"Validation Precision = \", val_precision)\n",
    "print(\"Validation Recall = \", val_recall)\n",
    "print(\"Validation f1 = \", val_f1)\n",
    "print(\"Validation AUC = \", val_aucroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effca129-7cc3-4d85-9a4c-5a55dab393f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_predicted_prob = val_predicted_prob.reshape((int(len(val_predicted_prob)/12),12))\n",
    "val_predicted_combination = np.argmax(val_predicted_prob,axis=1)\n",
    "    \n",
    "scores = np.zeros(len(which_combination_even))\n",
    "zipped = list(zip(which_combination_even.tolist(), val_predicted_combination.tolist()))\n",
    "for i in range(len(which_combination_even)):\n",
    "    scores[i] = evaluation_matrix[zipped[i]]\n",
    "        \n",
    "score = sum(scores)/len(scores)\n",
    "print(\"Validation Jet Score = \", score)\n",
    "\n",
    "print(\"How many events are 100% correct: \", sum(scores==1)/len(scores), \", Random = \",sum(evaluation_matrix[0,:]==1)/12)\n",
    "print(\"How many events are 50% correct: \", sum(scores==0.5)/len(scores), \", Random = \",sum(evaluation_matrix[0,:]==0.5)/12)\n",
    "print(\"How many events are 25% correct: \", sum(scores==0.25)/len(scores), \", Random = \",sum(evaluation_matrix[0,:]==0.25)/12)\n",
    "print(\"How many events are 0% correct: \", sum(scores==0)/len(scores), \", Random = \",sum(evaluation_matrix[0,:]==0)/12)\n",
    "\n",
    "train_predicted_prob = train_predicted_prob.reshape((int(len(train_predicted_prob)/12),12))\n",
    "train_predicted_combination = np.argmax(train_predicted_prob,axis=1)\n",
    "\n",
    "scores = np.zeros(len(which_combination_odd))\n",
    "zipped = list(zip(which_combination_odd.tolist(), train_predicted_combination.tolist()))\n",
    "for i in range(len(which_combination_odd)):\n",
    "    scores[i] = evaluation_matrix[zipped[i]]\n",
    "        \n",
    "score = sum(scores)/len(scores)\n",
    "print(\"Training Jet Score = \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95b2899-2b5d-47bb-b9f3-ff4df8811c5f",
   "metadata": {},
   "source": [
    "### m_bjj test (Compare BDT output to previous method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8307b1-d0b1-4f7e-9b93-841eeb257693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model_even = XGBClassifier()\n",
    "best_model_even.load_model(f\"models/model_{datetime.datetime.today().strftime('%y%m%d')}_even.json\")\n",
    "best_model_odd = XGBClassifier()\n",
    "best_model_odd.load_model(f\"models/model_{datetime.datetime.today().strftime('%y%m%d')}_odd.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a260e69-893f-4302-a3b1-857ece676486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grab features and labels and convert to np array\n",
    "features = output['features'].value\n",
    "labels = output['labels'].value\n",
    "even = output['even'].value\n",
    "observable = output['observable'].value\n",
    "even = np.repeat(even, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8235b5-98ea-4603-80df-e8d09aa01780",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_even = features[even]\n",
    "labels_even = labels[even]\n",
    "features_odd = features[np.invert(even)]\n",
    "labels_odd = labels[np.invert(even)]\n",
    "\n",
    "labels_even = labels_even.reshape((len(labels_even),))\n",
    "labels_odd = labels_odd.reshape((len(labels_odd),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ece407f-c953-476b-b07c-724720d9213e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_even_reshaped = features_even.reshape((int(len(features_even)/12),12,20))\n",
    "top_mass_candidates_even = features_even_reshaped[:,:,6]\n",
    "features_odd_reshaped = features_odd.reshape((int(len(features_odd)/12),12,20))\n",
    "top_mass_candidates_odd = features_odd_reshaped[:,:,6]\n",
    "\n",
    "observable_list = observable.astype(np.float32).tolist()\n",
    "all_correct_top_mass_even = features_even[labels_even==1,6]\n",
    "all_correct_top_mass_odd = features_odd[labels_odd==1,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96a4e3b-2077-4db5-b2f7-d29169dae5a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### mass histogram ####\n",
    "\n",
    "# binning\n",
    "combinedmass_low = 0.0\n",
    "combinedmass_high = 1500.0\n",
    "combinedmass_numbins = 200\n",
    "\n",
    "legendlist=[\"Truth\",\"Jet Triplet with Largest pT\",\"BDT\"]\n",
    "\n",
    "# define histogram\n",
    "h = hist.Hist(\n",
    "    hist.axis.Regular(combinedmass_numbins, combinedmass_low, combinedmass_high, \n",
    "                      name=\"combinedmass\", label=\"Reconstructed Top Mass [GeV]\", flow=False),\n",
    "    hist.axis.StrCategory(legendlist, name=\"truthlabel\", label=\"Truth Label\", growth=True),\n",
    ")\n",
    "\n",
    "# fill histogram\n",
    "h.fill(combinedmass = all_correct_top_mass_even, truthlabel=\"Truth\")\n",
    "h.fill(combinedmass = all_correct_top_mass_odd, truthlabel=\"Truth\")\n",
    "h.fill(combinedmass = observable_list, truthlabel=\"Jet Triplet with Largest pT\")\n",
    "\n",
    "# fill in odd predictions\n",
    "predictions = best_model_even.predict_proba(features_odd)[:,0]\n",
    "predictions = predictions.reshape((int(len(predictions)/12),12))\n",
    "which_combination = np.argmin(predictions,axis=-1)\n",
    "    \n",
    "top_mass_odd = np.zeros(features_odd_reshaped.shape[0])\n",
    "for j in range(len(top_mass_odd)):\n",
    "    top_mass_odd[j] = top_mass_candidates_odd[j,which_combination[j]]\n",
    "        \n",
    "h.fill(combinedmass = top_mass_odd, truthlabel=\"BDT\")\n",
    "\n",
    "# fill in even predictions\n",
    "predictions = best_model_odd.predict_proba(features_even)[:,0]\n",
    "predictions = predictions.reshape((int(len(predictions)/12),12))\n",
    "which_combination = np.argmin(predictions,axis=-1)\n",
    "    \n",
    "top_mass_even = np.zeros(features_even_reshaped.shape[0])\n",
    "for j in range(len(top_mass_even)):\n",
    "    top_mass_even[j] = top_mass_candidates_even[j,which_combination[j]]\n",
    "        \n",
    "h.fill(combinedmass = top_mass_even, truthlabel=\"BDT\")\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,8))\n",
    "h.plot(ax=ax)\n",
    "ax.legend(legendlist)\n",
    "ax.set_title(\"Reconstructed Top Mass\")\n",
    "ax.set_xlim([80,500])\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
