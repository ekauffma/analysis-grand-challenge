{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d27aa765-c7b7-4697-bae7-2ab0d1613ccd",
   "metadata": {},
   "source": [
    "# ttbar Analysis - Jet-Parton Assignment Training\n",
    "\n",
    "This is the training notebook for the jet-parton assignment task. The goal is to associate the leading four jets in each event to their associated parent particles. We are trying to assign jets according to the labels in the diagram below:\n",
    "\n",
    "<img src=\"utils/ttbar.png\" alt=\"ttbar_labels\" width=\"500\"/>\n",
    "\n",
    "top1 and top2 jets do not necessarily correspond to top/antitop, respectively. The top1 jet is defined as having a lepton/neutrino pair as cousins, where the top2 jet is defined as having two jets as cousins. The W jets are not distinguished from each other.\n",
    "\n",
    "The strategy for solving this problem is to train a boosted decision tree to find the correct assignments for each jet. Since we consider four jets per event with three unique labels (W, top1, and top2), there are twelve possible combinations of assignments:\n",
    "\n",
    "<img src=\"utils/jetcombinations.png\" alt=\"jetcombinations\" width=\"700\"/>\n",
    "\n",
    "The combination with the highest BDT score will be selected for each event.\n",
    "____\n",
    "\n",
    "The workflow for this training notebook is outlined as follows:\n",
    "* Load data and calculate training features and labels using `coffea`/`dask`\n",
    "* Optimize BDT (`xgboost` model) using `hyperopt` (TODO: Track using `mlflow`)\n",
    "* Save best model (TODO: save to `onnx`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37255f8f-4c05-426f-aaaf-86907762c585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import vector; vector.register_awkward()\n",
    "\n",
    "from coffea.nanoevents import NanoAODSchema\n",
    "from coffea import processor\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import hist\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import uproot\n",
    "\n",
    "import utils\n",
    "\n",
    "from dask.distributed import Client\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import ParameterSampler, train_test_split\n",
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "from mlflow.tracking import MlflowClient\n",
    "from xgboost import XGBClassifier\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9614d07-c1a3-49d5-96cd-c68114d4d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GLOBAL CONFIGURATION\n",
    "\n",
    "# input files per process, set to e.g. 10 (smaller number = faster, want to use larger number for training)\n",
    "N_FILES_MAX_PER_SAMPLE = 1\n",
    "# set to \"dask\" for DaskExecutor, \"futures\" for FuturesExecutor\n",
    "EXEC = \"futures\"\n",
    "\n",
    "# number of cores if using FuturesExecutor\n",
    "NUM_CORES = 4\n",
    "\n",
    "# chunk size to use\n",
    "CHUNKSIZE = 100_000\n",
    "\n",
    "# analysis facility: set to \"coffea_casa\" for coffea-casa environments, \"EAF\" for FNAL, \"local\" for local setups\n",
    "AF = \"coffea_casa\"\n",
    "\n",
    "# optional mlflow logging\n",
    "USE_MLFLOW = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250db8f8-1248-405a-9f02-5beb926f91cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "permutations_dict, labels_dict = utils.get_permutations_dict(4, include_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb36df2-f216-40b9-b62c-504ef9079f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these matrices tell you the overlap between the predicted label (rows) and truth label (columns)\n",
    "# the \"score\" in each matrix entry is the number of jets which are assigned correctly\n",
    "evaluation_matrices = {} # overall event score\n",
    "\n",
    "for n in range(4,4+1):\n",
    "    print(\"n = \", n)\n",
    "    evaluation_matrix = np.zeros((len(permutations_dict[n]),len(permutations_dict[n])))\n",
    "    \n",
    "    for i in range(len(permutations_dict[n])):\n",
    "        for j in range(len(permutations_dict[n])):\n",
    "            evaluation_matrix[i,j]=sum(np.equal(labels_dict[n][i], labels_dict[n][j]))\n",
    "    \n",
    "    evaluation_matrices[n] = evaluation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1a1ccb-c55b-4da6-9ea3-e2507a5db2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions for calculating features and labels for the BDT\n",
    "def training_filter(jets, electrons, muons, genparts, even):\n",
    "    '''\n",
    "    Filters events down to training set and calculates jet-level labels\n",
    "    \n",
    "    Args:\n",
    "        jets: selected jets after region filter (and selecting leading four for each event)\n",
    "        electrons: selected electrons after region filter\n",
    "        muons: selected muons after region filter\n",
    "        genparts: selected genpart after region filter\n",
    "    \n",
    "    Returns:\n",
    "        jets, electrons, muons, labels\n",
    "    '''\n",
    "    #### filter genPart to valid matching candidates ####\n",
    "\n",
    "    # get rid of particles without parents\n",
    "    genpart_parent = genparts.distinctParent\n",
    "    genpart_filter = np.invert(ak.is_none(genpart_parent, axis=1))\n",
    "    genparts = genparts[genpart_filter]\n",
    "    genpart_parent = genparts.distinctParent\n",
    "\n",
    "    # ensure that parents are top quark or W\n",
    "    genpart_filter2 = ((np.abs(genpart_parent.pdgId)==6) | (np.abs(genpart_parent.pdgId)==24))\n",
    "    genparts = genparts[genpart_filter2]\n",
    "\n",
    "    # ensure particle itself is a quark\n",
    "    genpart_filter3 = ((np.abs(genparts.pdgId)<7) & (np.abs(genparts.pdgId)>0))\n",
    "    genparts = genparts[genpart_filter3]\n",
    "\n",
    "    # get rid of duplicates\n",
    "    genpart_filter4 = genparts.hasFlags(\"isLastCopy\")\n",
    "    genparts = genparts[genpart_filter4]\n",
    "            \n",
    "        \n",
    "    #### get jet-level labels and filter events to training set\n",
    "        \n",
    "    # match jets to nearest valid genPart candidate\n",
    "    nearest_genpart = jets.nearest(genparts, threshold=0.4)\n",
    "    nearest_parent = nearest_genpart.distinctParent # parent of matched particle\n",
    "\n",
    "    parent_pdgid = nearest_parent.pdgId # pdgId of parent particle\n",
    "    grandchild_pdgid = nearest_parent.distinctChildren.distinctChildren.pdgId # pdgId of particle's parent's grandchildren\n",
    "\n",
    "    grandchildren_flat = np.abs(ak.flatten(grandchild_pdgid,axis=-1)) # flatten innermost axis for convenience\n",
    "\n",
    "    # if particle has a cousin that is a lepton\n",
    "    has_lepton_cousin = (ak.sum(((grandchildren_flat%2==0) & (grandchildren_flat>10) & (grandchildren_flat<19)),\n",
    "                                axis=-1)>0)\n",
    "    # if particle has a cousin that is a neutrino\n",
    "    has_neutrino_cousin = (ak.sum(((grandchildren_flat%2==1) & (grandchildren_flat>10) & (grandchildren_flat<19)),\n",
    "                                  axis=-1)>0)\n",
    "\n",
    "    # if a particle has a lepton cousin and a neutrino cousin\n",
    "    has_both_cousins = ak.fill_none((has_lepton_cousin & has_neutrino_cousin), False).to_numpy()\n",
    "\n",
    "    # get labels from parent pdgId (fill none with 100 to filter out events with those jets)\n",
    "    labels = np.abs(ak.fill_none(parent_pdgid,100).to_numpy())\n",
    "    labels[has_both_cousins] = -6 # assign jets with both cousins as top1 (not necessarily antiparticle)\n",
    "\n",
    "    training_event_filter = (np.sum(labels,axis=1)==48) # events with a label sum of 48 have the correct particles\n",
    "            \n",
    "    # filter events\n",
    "    jets = jets[training_event_filter]\n",
    "    electrons = electrons[training_event_filter]\n",
    "    muons = muons[training_event_filter]\n",
    "    labels = labels[training_event_filter]\n",
    "    even = even[training_event_filter]\n",
    "    \n",
    "    return jets, electrons, muons, labels, even\n",
    "    \n",
    "\n",
    "def get_training_set(jets, electrons, muons, labels, permutations_dict, labels_dict):\n",
    "    '''\n",
    "    Calculate features for each of the 12 combinations per event and calculates combination-level labels\n",
    "    \n",
    "    Args:\n",
    "        jets: selected jets after training filter\n",
    "        electrons: selected electrons after training filter\n",
    "        muons: selected muons after training filter\n",
    "        labels: jet-level labels output by training_filter\n",
    "    \n",
    "    Returns:\n",
    "        features, labels (flattened to remove event level)\n",
    "    '''\n",
    "    \n",
    "    # calculate number of jets in each event\n",
    "    njet = ak.num(jets).to_numpy()\n",
    "    # don't consider every jet for events with high jet multiplicity\n",
    "    njet[njet>max(permutations_dict.keys())] = max(permutations_dict.keys())\n",
    "    # create awkward array of permutation indices\n",
    "    perms = ak.Array([permutations_dict[n] for n in njet])\n",
    "    perm_counts = ak.num(perms)\n",
    "    \n",
    "    \n",
    "    #### calculate features ####\n",
    "    \n",
    "    #### calculate features ####\n",
    "    features = np.zeros((sum(perm_counts),19))\n",
    "    \n",
    "    # grab lepton info\n",
    "    leptons = ak.flatten(ak.concatenate((electrons, muons),axis=1),axis=-1)\n",
    "\n",
    "    # delta R between top1 and lepton\n",
    "    features[:,0] = ak.flatten(np.sqrt((leptons.eta - jets[perms[...,3]].eta)**2 + \n",
    "                                       (leptons.phi - jets[perms[...,3]].phi)**2)).to_numpy()\n",
    "\n",
    "    # delta R between the two W\n",
    "    features[:,1] = ak.flatten(np.sqrt((jets[perms[...,0]].eta - jets[perms[...,1]].eta)**2 + \n",
    "                                       (jets[perms[...,0]].phi - jets[perms[...,1]].phi)**2)).to_numpy()\n",
    "\n",
    "    # delta R between W and top2\n",
    "    features[:,2] = ak.flatten(np.sqrt((jets[perms[...,0]].eta - jets[perms[...,2]].eta)**2 + \n",
    "                                       (jets[perms[...,0]].phi - jets[perms[...,2]].phi)**2)).to_numpy()\n",
    "    features[:,3] = ak.flatten(np.sqrt((jets[perms[...,1]].eta - jets[perms[...,2]].eta)**2 + \n",
    "                                       (jets[perms[...,1]].phi - jets[perms[...,2]].phi)**2)).to_numpy()\n",
    "\n",
    "    # delta phi between top1 and lepton\n",
    "    features[:,4] = ak.flatten(np.abs(leptons.phi - jets[perms[...,3]].phi)).to_numpy()\n",
    "\n",
    "    # delta phi between the two W\n",
    "    features[:,5] = ak.flatten(np.abs(jets[perms[...,0]].phi - jets[perms[...,1]].phi)).to_numpy()\n",
    "\n",
    "    # delta phi between W and top2\n",
    "    features[:,6] = ak.flatten(np.abs(jets[perms[...,0]].phi - jets[perms[...,2]].phi)).to_numpy()\n",
    "    features[:,7] = ak.flatten(np.abs(jets[perms[...,1]].phi - jets[perms[...,2]].phi)).to_numpy()\n",
    "\n",
    "\n",
    "    # combined mass of top1 and lepton\n",
    "    features[:,8] = ak.flatten((leptons + jets[perms[...,3]]).mass).to_numpy()\n",
    "\n",
    "    # combined mass of W\n",
    "    features[:,9] = ak.flatten((jets[perms[...,0]] + jets[perms[...,1]]).mass).to_numpy()\n",
    "\n",
    "    # combined mass of W and top2\n",
    "    features[:,10] = ak.flatten((jets[perms[...,0]] + jets[perms[...,1]] + \n",
    "                                 jets[perms[...,2]]).mass).to_numpy()\n",
    "\n",
    "\n",
    "    # pt of every jet\n",
    "    features[:,11] = ak.flatten(jets[perms[...,0]].pt).to_numpy()\n",
    "    features[:,12] = ak.flatten(jets[perms[...,1]].pt).to_numpy()\n",
    "    features[:,13] = ak.flatten(jets[perms[...,2]].pt).to_numpy()\n",
    "    features[:,14] = ak.flatten(jets[perms[...,3]].pt).to_numpy()\n",
    "\n",
    "\n",
    "    # mass of every jet\n",
    "    features[:,15] = ak.flatten(jets[perms[...,0]].mass).to_numpy()\n",
    "    features[:,16] = ak.flatten(jets[perms[...,1]].mass).to_numpy()\n",
    "    features[:,17] = ak.flatten(jets[perms[...,2]].mass).to_numpy()\n",
    "    features[:,18] = ak.flatten(jets[perms[...,3]].mass).to_numpy()\n",
    "    \n",
    "    #### calculate combination-level labels ####\n",
    "    permutation_labels = np.array(labels_dict[4])\n",
    "    \n",
    "    # which combination does the truth label correspond to?\n",
    "    which_combination = np.zeros(len(jets), dtype=int)\n",
    "    # no correct matches\n",
    "    which_anti_combination = np.zeros(labels.shape[0], dtype=int)\n",
    "    for i in range(12):\n",
    "        which_combination[(labels==permutation_labels[i,:]).all(1)] = i\n",
    "        which_anti_combination[np.invert((labels==permutation_labels[i,:]).any(1))] = i\n",
    "\n",
    "    # convert to combination-level truth label (-1, 0 or 1)\n",
    "    which_combination = list(zip(range(len(jets),), which_combination))\n",
    "    which_anti_combination = list(zip(range(labels.shape[0],), which_anti_combination))\n",
    "    \n",
    "    truth_labels = -1*np.ones((len(jets),12))\n",
    "    for i,tpl in enumerate(which_combination):\n",
    "        truth_labels[tpl]=1\n",
    "    for i,tpl in enumerate(which_anti_combination):\n",
    "        truth_labels[tpl]=0\n",
    "        \n",
    "        \n",
    "    #### flatten to combinations (easy to unflatten since each event always has 12 combinations) ####\n",
    "    labels = truth_labels.reshape((truth_labels.shape[0]*truth_labels.shape[1],1))\n",
    "    \n",
    "    return features, labels, which_combination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1112746b-9802-4895-88a6-707b62732fb7",
   "metadata": {},
   "source": [
    "### Defining a `coffea` Processor\n",
    "\n",
    "The processor returns the training features and labels we will use in our BDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a311fee4-b2d1-4d26-b076-047754aa45bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_base = processor.ProcessorABC\n",
    "class JetClassifier(processor_base):\n",
    "    def __init__(self, permutations_dict, labels_dict):\n",
    "        super().__init__()\n",
    "        self.permutations_dict = permutations_dict\n",
    "        self.labels_dict = labels_dict\n",
    "    \n",
    "    def process(self, events):\n",
    "        \n",
    "        process = events.metadata[\"process\"]  # \"ttbar\" etc.\n",
    "        variation = events.metadata[\"variation\"]  # \"nominal\" etc.\n",
    "        \n",
    "        # normalization for MC\n",
    "        x_sec = events.metadata[\"xsec\"]\n",
    "        nevts_total = events.metadata[\"nevts\"]\n",
    "        lumi = 3378 # /pb\n",
    "        xsec_weight = x_sec * lumi / nevts_total\n",
    "            \n",
    "        events[\"pt_nominal\"] = 1.0\n",
    "        pt_variations = [\"pt_nominal\"] if variation == \"nominal\" else [\"pt_nominal\"]\n",
    "        for pt_var in pt_variations:\n",
    "            \n",
    "            # filter electrons, muons, and jets by pT\n",
    "            selected_electrons = events.Electron[(events.Electron.pt > 30) & (np.abs(events.Electron.eta)<2.1) & \n",
    "                                                 (events.Electron.cutBased==4) & (events.Electron.sip3d < 4)]\n",
    "            selected_muons = events.Muon[(events.Muon.pt > 30) & (np.abs(events.Muon.eta)<2.1) & (events.Muon.tightId) & \n",
    "                                         (events.Muon.sip3d < 4) & (events.Muon.pfRelIso04_all < 0.15)]\n",
    "            jet_filter = (events.Jet.pt > 30) & (np.abs(events.Jet.eta) < 2.4)\n",
    "            selected_jets = events.Jet[jet_filter]\n",
    "            selected_genpart = events.GenPart\n",
    "            even = (events.event%2==0)\n",
    "            \n",
    "            # single lepton requirement\n",
    "            event_filters = ((ak.count(selected_electrons.pt, axis=1) + ak.count(selected_muons.pt, axis=1)) == 1)\n",
    "            # require at least 4 jets\n",
    "            event_filters = event_filters & (ak.count(selected_jets.pt, axis=1) >= 4)\n",
    "            # require at least one jet above B_TAG_THRESHOLD\n",
    "            B_TAG_THRESHOLD = 0.5\n",
    "            event_filters = event_filters & (ak.sum(selected_jets.btagCSVV2 >= B_TAG_THRESHOLD, axis=1) >= 1)\n",
    "            \n",
    "            # apply event filters\n",
    "            selected_events = events[event_filters]\n",
    "            selected_electrons = selected_electrons[event_filters]\n",
    "            selected_muons = selected_muons[event_filters]\n",
    "            selected_jets = selected_jets[event_filters]\n",
    "            selected_genpart = selected_genpart[event_filters]\n",
    "            even = even[event_filters]\n",
    "            \n",
    "            ### only consider 4j2b region\n",
    "            region_filter = ak.sum(selected_jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 2 # at least two b-tagged jets\n",
    "            selected_jets_region = selected_jets[region_filter][:,:4] # only keep top 4 jets\n",
    "            selected_electrons_region = selected_electrons[region_filter]\n",
    "            selected_muons_region = selected_muons[region_filter]\n",
    "            selected_genpart_region = selected_genpart[region_filter]\n",
    "            even = even[region_filter]\n",
    "            \n",
    "            # filter events and calculate labels\n",
    "            jets, electrons, muons, labels, even = training_filter(selected_jets_region, \n",
    "                                                                   selected_electrons_region, \n",
    "                                                                   selected_muons_region, \n",
    "                                                                   selected_genpart_region,\n",
    "                                                                   even)\n",
    "            \n",
    "            # calculate features and labels\n",
    "            features, labels, which_combination = get_training_set(jets, electrons, muons, labels,\n",
    "                                                                   self.permutations_dict, self.labels_dict)\n",
    "    \n",
    "            # calculate mbjj\n",
    "            # reconstruct hadronic top as bjj system with largest pT\n",
    "            # the jet energy scale / resolution effect is not propagated to this observable at the moment\n",
    "            trijet = ak.combinations(jets, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidates\n",
    "            trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # calculate four-momentum of tri-jet system\n",
    "            trijet[\"max_btag\"] = np.maximum(trijet.j1.btagCSVV2, np.maximum(trijet.j2.btagCSVV2, trijet.j3.btagCSVV2))\n",
    "            trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # at least one-btag in trijet candidates\n",
    "            # pick trijet candidate with largest pT and calculate mass of system\n",
    "            trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "            observable = ak.flatten(trijet_mass)\n",
    "            \n",
    "        output = {\"nevents\": {events.metadata[\"dataset\"]: len(events)},\n",
    "                  \"features\": {events.metadata[\"dataset\"]: features.tolist()},\n",
    "                  \"labels\": {events.metadata[\"dataset\"]: labels.tolist()},\n",
    "                  \"observable\": {events.metadata[\"dataset\"]: observable.to_list()},\n",
    "                  \"even\": {events.metadata[\"dataset\"]: even.to_list()}}\n",
    "            \n",
    "        return output\n",
    "        \n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4c24f0-b45f-4c5e-a0ce-dc6afd960e74",
   "metadata": {},
   "source": [
    "### \"Fileset\" construction and metadata\n",
    "\n",
    "Here, we gather all the required information about the files we want to process: paths to the files and asociated metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45fc2b2-b966-4d74-ac33-342490d900b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileset = utils.construct_fileset(N_FILES_MAX_PER_SAMPLE, \n",
    "                                  use_xcache=False, \n",
    "                                  json_file = 'ntuples_nanoaod_agc.json')\n",
    "\n",
    "# get rid of everything except ttbar__nominal for training purposes\n",
    "fileset_keys = list(fileset.keys())\n",
    "for key in fileset_keys:\n",
    "    if key!=\"ttbar__nominal\":\n",
    "        fileset.pop(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7513a6ea-b49d-4ee1-8d3d-bee7e48c7467",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "fileset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b15da9-2f6d-4fa2-8c67-6e924360b04e",
   "metadata": {},
   "source": [
    "### Execute the data delivery pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d4acb1-816b-408a-8a91-e5ebbd99e340",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "NanoAODSchema.warn_missing_crossrefs = False\n",
    "\n",
    "if EXEC == \"futures\":\n",
    "    executor = processor.FuturesExecutor(workers=NUM_CORES)\n",
    "elif EXEC == \"dask\":\n",
    "    executor = processor.DaskExecutor(client=utils.get_client(AF))\n",
    "    \n",
    "run = processor.Runner(executor=executor, schema=NanoAODSchema, savemetrics=True, metadata_cache={}, \n",
    "                       chunksize=CHUNKSIZE)\n",
    "\n",
    "# preprocess\n",
    "filemeta = run.preprocess(fileset, treename=\"Events\")\n",
    "\n",
    "# process\n",
    "output, metrics = run(fileset, \n",
    "                      \"Events\", \n",
    "                      processor_instance = JetClassifier(permutations_dict, labels_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294f56e7-71d2-4ad5-b2f6-bd713629ffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(output, open(\"output_temp.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b51e1a-05da-4ab6-bcf1-cf2e120e0880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "output = pickle.load(open(\"output_temp.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3085d626-09dc-4548-82cd-92793aec1237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab features and labels and convert to np array\n",
    "features = np.array(output['features']['ttbar__nominal'])\n",
    "labels = np.array(output['labels']['ttbar__nominal'])\n",
    "even = np.array(output['even']['ttbar__nominal'])\n",
    "\n",
    "labels = labels.reshape((len(labels),))\n",
    "even = np.repeat(even, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42764f0-04fa-4701-b5bb-cf383bdbea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate labels\n",
    "print(len(labels))\n",
    "print(len(labels)/12)\n",
    "print(sum(labels==1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0859a175-ace2-4ce8-9934-92cbdda30028",
   "metadata": {},
   "source": [
    "The key for the labeling scheme is as follows\n",
    "\n",
    "* 1: all jet assignments are correct\n",
    "* 0: some jet assignments are correct (one or two are correct, others are incorrect)\n",
    "* -1: all jet assignments are incorrect\n",
    "\n",
    "There are twelve combinations for each event, so each event will have 1 correct combination, 2 completely incorrect combinations, and 9 partially correct combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5523c438-d438-4acb-9c49-19a4272909ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate by label for plotting\n",
    "all_correct = features[labels==1,:]\n",
    "some_correct = features[labels==-1,:]\n",
    "none_correct = features[labels==0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d532f2-ef1c-4ea1-922f-dd1259bd91a9",
   "metadata": {},
   "source": [
    "# Histograms of Training Variables\n",
    "To vizualize the separation power of the different variables, histograms are created for each of the three labels. Only `all_correct` and `none_correct` are used for training purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f52735-1a56-4346-8040-ef7db1037c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### delta R histogram ####\n",
    "\n",
    "# binning\n",
    "deltar_low = 0.0\n",
    "deltar_high = 8.0\n",
    "deltar_numbins = 100\n",
    "legend_list = [\"All Matches Correct\", \"Some Matches Correct\", \"No Matches Correct\"]\n",
    "\n",
    "# define histogram\n",
    "h = hist.Hist(\n",
    "    hist.axis.Regular(deltar_numbins, deltar_low, deltar_high, name=\"deltar\", label=\"$\\Delta R$\", flow=False),\n",
    "    hist.axis.StrCategory(legend_list, name=\"truthlabel\", label=\"Truth Label\"),\n",
    "    hist.axis.StrCategory([\"top1_lepton\",\"W_W\",\"top2_W\"], name=\"category\", label=\"Category\"),\n",
    ")\n",
    "\n",
    "# fill histogram\n",
    "h.fill(deltar = all_correct[:,0], category=\"top1_lepton\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(deltar = some_correct[:,0], category=\"top1_lepton\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(deltar = none_correct[:,0], category=\"top1_lepton\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(deltar = all_correct[:,1], category=\"W_W\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(deltar = some_correct[:,1], category=\"W_W\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(deltar = none_correct[:,1], category=\"W_W\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(deltar = all_correct[:,2], category=\"top2_W\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(deltar = some_correct[:,2], category=\"top2_W\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(deltar = none_correct[:,2], category=\"top2_W\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(deltar = all_correct[:,3], category=\"top2_W\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(deltar = some_correct[:,3], category=\"top2_W\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(deltar = none_correct[:,3], category=\"top2_W\", truthlabel=\"No Matches Correct\")\n",
    "\n",
    "# make plots\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[0j::hist.rebin(2), :, \"top1_lepton\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list)\n",
    "ax.set_title(\"$\\Delta R$ between top1 jet and lepton\")\n",
    "fig.show()\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[0j::hist.rebin(2), :, \"W_W\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list)\n",
    "ax.set_title(\"$\\Delta R$ between the two W jets\")\n",
    "fig.show()\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[0j::hist.rebin(2), :, \"top2_W\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list)\n",
    "ax.set_title(\"$\\Delta R$ between W jet and top2 jet\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89daf906-d669-455d-917e-5fb79f41e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### delta phi histogram ####\n",
    "\n",
    "# binning\n",
    "deltaphi_low = 0.0\n",
    "deltaphi_high = 2*np.pi\n",
    "deltaphi_numbins = 100\n",
    "legend_list = [\"All Matches Correct\", \"Some Matches Correct\", \"No Matches Correct\"]\n",
    "\n",
    "# define histogram\n",
    "h = hist.Hist(\n",
    "    hist.axis.Regular(deltaphi_numbins, deltaphi_low, deltaphi_high, name=\"deltaphi\", label=\"$\\Delta \\phi$\", flow=False),\n",
    "    hist.axis.StrCategory(legend_list, name=\"truthlabel\", label=\"Truth Label\"),\n",
    "    hist.axis.StrCategory([\"top1_lepton\",\"W_W\",\"top2_W\"], name=\"category\", label=\"Category\"),\n",
    ")\n",
    "\n",
    "# fill histogram\n",
    "h.fill(deltaphi = all_correct[:,4], category=\"top1_lepton\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(deltaphi = some_correct[:,4], category=\"top1_lepton\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(deltaphi = none_correct[:,4], category=\"top1_lepton\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(deltaphi = all_correct[:,5], category=\"W_W\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(deltaphi = some_correct[:,5], category=\"W_W\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(deltaphi = none_correct[:,5], category=\"W_W\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(deltaphi = all_correct[:,6], category=\"top2_W\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(deltaphi = some_correct[:,6], category=\"top2_W\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(deltaphi = none_correct[:,6], category=\"top2_W\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(deltaphi = all_correct[:,7], category=\"top2_W\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(deltaphi = some_correct[:,7], category=\"top2_W\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(deltaphi = none_correct[:,7], category=\"top2_W\", truthlabel=\"No Matches Correct\")\n",
    "\n",
    "# make plots\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[0j::hist.rebin(2), :, \"top1_lepton\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list)\n",
    "ax.set_title(\"$\\Delta \\phi$ between top1 jet and lepton\")\n",
    "fig.show()\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[0j::hist.rebin(2), :, \"W_W\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list)\n",
    "ax.set_title(\"$\\Delta \\phi$ between the two W jets\")\n",
    "fig.show()\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[0j::hist.rebin(2), :, \"top2_W\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list)\n",
    "ax.set_title(\"$\\Delta \\phi$ between W jet and top2 jet\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb40ada0-8acd-4882-b489-49bf3eed16e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### mass histogram ####\n",
    "\n",
    "# binning\n",
    "combinedmass_low = 0.0\n",
    "combinedmass_high = 1500.0\n",
    "combinedmass_numbins = 200\n",
    "legend_list = [\"All Matches Correct\", \"Some Matches Correct\", \"No Matches Correct\", \"Jet Triplet with Largest pT\"]\n",
    "\n",
    "# define histogram\n",
    "h = hist.Hist(\n",
    "    hist.axis.Regular(combinedmass_numbins, combinedmass_low, combinedmass_high, \n",
    "                      name=\"combinedmass\", label=\"Combined Mass [GeV]\", flow=False),\n",
    "    hist.axis.StrCategory(legend_list, name=\"truthlabel\", label=\"Truth Label\"),\n",
    "    hist.axis.StrCategory([\"top1_lepton\",\"W_W\",\"top2_W_W\"], name=\"category\", label=\"Category\"),\n",
    ")\n",
    "\n",
    "# fill histogram\n",
    "h.fill(combinedmass = all_correct[:,8], category=\"top1_lepton\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(combinedmass = some_correct[:,8], category=\"top1_lepton\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(combinedmass = none_correct[:,8], category=\"top1_lepton\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(combinedmass = all_correct[:,9], category=\"W_W\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(combinedmass = some_correct[:,9], category=\"W_W\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(combinedmass = none_correct[:,9], category=\"W_W\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(combinedmass = all_correct[:,10], category=\"top2_W_W\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(combinedmass = some_correct[:,10], category=\"top2_W_W\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(combinedmass = none_correct[:,10], category=\"top2_W_W\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(combinedmass = output[\"observable\"][\"ttbar__nominal\"], category=\"top2_W_W\", truthlabel=\"Jet Triplet with Largest pT\")\n",
    "\n",
    "# make plots\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[:, :, \"top1_lepton\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list[:-1])\n",
    "ax.set_title(\"Combined mass of top1 jet and lepton\")\n",
    "ax.set_xlim([0,400])\n",
    "fig.show()\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[:, :, \"W_W\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list[:-1])\n",
    "ax.set_title(\"Combined mass of the two W jets\")\n",
    "ax.set_xlim([0,400])\n",
    "fig.show()\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[:, :, \"top2_W_W\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list)\n",
    "ax.set_title(\"Combined mass of W jets and top2 jet\")\n",
    "ax.set_xlim([0,600])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f25b60-a955-4534-a777-9c3c734382d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### pT histogram ####\n",
    "\n",
    "# binning\n",
    "pt_low = 25.0\n",
    "pt_high = 300.0\n",
    "pt_numbins = 100\n",
    "legend_list = [\"All Matches Correct\", \"Some Matches Correct\", \"No Matches Correct\"]\n",
    "\n",
    "# define histogram\n",
    "h = hist.Hist(\n",
    "    hist.axis.Regular(pt_numbins, pt_low, pt_high, \n",
    "                      name=\"jetpt\", label=\"Jet $p_T$ [GeV]\", flow=False),\n",
    "    hist.axis.StrCategory(legend_list, name=\"truthlabel\", label=\"Truth Label\"),\n",
    "    hist.axis.StrCategory([\"W\",\"top1\",\"top2\"], name=\"category\", label=\"Category\"),\n",
    ")\n",
    "\n",
    "# fill histogram\n",
    "h.fill(jetpt = all_correct[:,11], category=\"W\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(jetpt = some_correct[:,11], category=\"W\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(jetpt = none_correct[:,11], category=\"W\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(jetpt = all_correct[:,12], category=\"W\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(jetpt = some_correct[:,12], category=\"W\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(jetpt = none_correct[:,12], category=\"W\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(jetpt = all_correct[:,13], category=\"top2\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(jetpt = some_correct[:,13], category=\"top2\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(jetpt = none_correct[:,13], category=\"top2\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(jetpt = all_correct[:,14], category=\"top1\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(jetpt = some_correct[:,14], category=\"top1\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(jetpt = none_correct[:,14], category=\"top1\", truthlabel=\"No Matches Correct\")\n",
    "\n",
    "# make plots\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[:, :, \"W\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list)\n",
    "ax.set_title(\"W Jet $p_T$\")\n",
    "ax.set_xlim([25,300])\n",
    "fig.show()\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[:, :, \"top2\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list)\n",
    "ax.set_title(\"top2 Jet $p_T$\")\n",
    "ax.set_xlim([25,300])\n",
    "fig.show()\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[:, :, \"top1\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list)\n",
    "ax.set_title(\"top1 Jet $p_T$\")\n",
    "ax.set_xlim([25,200])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c007f4-3e93-4b74-b03e-0028c4fc85b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### mass histogram ####\n",
    "\n",
    "# binning\n",
    "mass_low = 0.0\n",
    "mass_high = 50.0\n",
    "mass_numbins = 100\n",
    "legend_list = [\"All Matches Correct\", \"Some Matches Correct\", \"No Matches Correct\"]\n",
    "\n",
    "# define histogram\n",
    "h = hist.Hist(\n",
    "    hist.axis.Regular(mass_numbins, mass_low, mass_high, \n",
    "                      name=\"jetmass\", label=\"Jet Mass [GeV]\", flow=False),\n",
    "    hist.axis.StrCategory(legend_list, name=\"truthlabel\", label=\"Truth Label\"),\n",
    "    hist.axis.StrCategory([\"W\",\"top1\",\"top2\"], name=\"category\", label=\"Category\"),\n",
    ")\n",
    "\n",
    "# fill histogram\n",
    "h.fill(jetmass = all_correct[:,15], category=\"W\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(jetmass = some_correct[:,15], category=\"W\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(jetmass = none_correct[:,15], category=\"W\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(jetmass = all_correct[:,16], category=\"W\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(jetmass = some_correct[:,16], category=\"W\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(jetmass = none_correct[:,16], category=\"W\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(jetmass = all_correct[:,17], category=\"top2\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(jetmass = some_correct[:,17], category=\"top2\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(jetmass = none_correct[:,17], category=\"top2\", truthlabel=\"No Matches Correct\")\n",
    "h.fill(jetmass = all_correct[:,18], category=\"top1\", truthlabel=\"All Matches Correct\")\n",
    "h.fill(jetmass = some_correct[:,18], category=\"top1\", truthlabel=\"Some Matches Correct\")\n",
    "h.fill(jetmass = none_correct[:,18], category=\"top1\", truthlabel=\"No Matches Correct\")\n",
    "\n",
    "# make plots\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[:, :, \"W\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list)\n",
    "ax.set_title(\"W Jet Mass\")\n",
    "# fig.show()\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[:, :, \"top2\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list)\n",
    "ax.set_title(\"top2 Jet Mass\")\n",
    "fig.show()\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(8,4))\n",
    "h[:, :, \"top1\"].plot(density=True, ax=ax)\n",
    "ax.legend(legend_list)\n",
    "ax.set_title(\"top1 Jet Mass\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a534fc84-a8c4-4ac0-8b18-14b12de6ed8e",
   "metadata": {},
   "source": [
    "# Model Optimization\n",
    "\n",
    "The model used here is `xgboost`'s gradient-boosted decision tree (`XGBClassifier`). Hyperparameter optimization is performed using random selection from a sample space of hyperparameters then testing model fits in a parallelized manner using `dask`. Optional `mlflow` logging is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590a526b-6848-472d-a299-0c768caafaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab features and labels and convert to np array\n",
    "features = np.array(output['features']['ttbar__nominal'])\n",
    "labels = np.array(output['labels']['ttbar__nominal'])\n",
    "even = np.array(output['even']['ttbar__nominal'])\n",
    "\n",
    "labels = labels.reshape((len(labels),))\n",
    "even = np.repeat(even, 12)\n",
    "\n",
    "labels[labels==-1]=0 # consider all combination (partially correct is same as 0% correct for training)\n",
    "\n",
    "features_even = features[even]\n",
    "labels_even = labels[even]\n",
    "\n",
    "features_odd = features[np.invert(even)]\n",
    "labels_odd = labels[np.invert(even)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52c0250-48ba-42ef-b3f5-39b8039dbba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### separate data into train/val/testr ###\n",
    "\n",
    "RANDOM_SEED = 5\n",
    "TRAIN_RATIO = 0.9\n",
    "\n",
    "# separate even into train/val\n",
    "features_even_unflattened = features_even.reshape((int(features_even.shape[0]/12),12,19))\n",
    "labels_even_unflattened = labels_even.reshape((int(features_even.shape[0]/12),12))\n",
    "\n",
    "features_train_even, features_val_even, labels_train_even, labels_val_even = train_test_split(features_even_unflattened, \n",
    "                                                                                              labels_even_unflattened, \n",
    "                                                                                              train_size=TRAIN_RATIO, \n",
    "                                                                                              random_state=RANDOM_SEED)\n",
    "\n",
    "print(\"features_train_even.shape = \", features_train_even.shape)\n",
    "print(\"labels_train_even.shape = \", labels_train_even.shape)\n",
    "print(\"features_val_even.shape = \", features_val_even.shape)\n",
    "print(\"labels_val_even.shape = \", labels_val_even.shape)\n",
    "\n",
    "which_combination_train_even = np.where(labels_train_even==1)[1]\n",
    "features_train_even = features_train_even.reshape((12*features_train_even.shape[0],19))\n",
    "labels_train_even = labels_train_even.reshape((12*labels_train_even.shape[0],))\n",
    "\n",
    "which_combination_val_even = np.where(labels_val_even==1)[1]\n",
    "features_val_even = features_val_even.reshape((12*features_val_even.shape[0],19))\n",
    "labels_val_even = labels_val_even.reshape((12*labels_val_even.shape[0],))\n",
    "\n",
    "\n",
    "# separate odd into train/val\n",
    "features_odd_unflattened = features_odd.reshape((int(features_odd.shape[0]/12),12,19))\n",
    "labels_odd_unflattened = labels_odd.reshape((int(features_odd.shape[0]/12),12))\n",
    "\n",
    "features_train_odd, features_val_odd, labels_train_odd, labels_val_odd = train_test_split(features_odd_unflattened, \n",
    "                                                                                          labels_odd_unflattened, \n",
    "                                                                                          train_size=TRAIN_RATIO, \n",
    "                                                                                          random_state=RANDOM_SEED)\n",
    "\n",
    "print(\"features_train_odd.shape = \", features_train_odd.shape)\n",
    "print(\"labels_train_odd.shape = \", labels_train_odd.shape)\n",
    "print(\"features_val_odd.shape = \", features_val_odd.shape)\n",
    "print(\"labels_val_odd.shape = \", labels_val_odd.shape)\n",
    "\n",
    "which_combination_train_odd = np.where(labels_train_odd==1)[1]\n",
    "features_train_odd = features_train_odd.reshape((12*features_train_odd.shape[0],19))\n",
    "labels_train_odd = labels_train_odd.reshape((12*labels_train_odd.shape[0],))\n",
    "\n",
    "which_combination_val_odd = np.where(labels_val_odd==1)[1]\n",
    "features_val_odd = features_val_odd.reshape((12*features_val_odd.shape[0],19))\n",
    "labels_val_odd = labels_val_odd.reshape((12*labels_val_odd.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ddf0f-9100-422a-9e38-c3b4cb9dfa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess features so that they are more Gaussian-like\n",
    "power = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "\n",
    "features_train_even = power.fit_transform(features_train_even)\n",
    "features_val_even = power.transform(features_val_even)\n",
    "features_train_odd = power.transform(features_train_odd)\n",
    "features_val_odd = power.transform(features_val_odd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfef7c10-42bb-43ff-89f5-541f95b3860a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = ParameterSampler({'max_depth': np.arange(2,30,2,dtype=int), \n",
    "                            'n_estimators': np.arange(50,700,20,dtype=int), \n",
    "                            'learning_rate': np.logspace(-5, -1, 10),\n",
    "                            'min_child_weight': np.logspace(-1, 2, 20), \n",
    "                            'reg_lambda': [0, 0.25, 0.5, 0.75, 1], \n",
    "                            'reg_alpha': [0, 0.25, 0.5, 0.75, 1],\n",
    "                            'gamma': np.logspace(-4, 1, 20),}, \n",
    "                            n_iter = 50, \n",
    "                            random_state=34) \n",
    "\n",
    "samples = list(sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1c1fe1-614a-425c-aae8-287c2a58304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(samples)):\n",
    "    samples[i]['trial_num'] = i\n",
    "samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20d3607-b3f6-4070-8f6f-bc7257d7236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(params, \n",
    "              features_train, \n",
    "              labels_train, \n",
    "              which_combination_train,\n",
    "              features_val, \n",
    "              labels_val, \n",
    "              which_combination_val,\n",
    "              evaluation_matrix,\n",
    "              USE_MLFLOW=False): \n",
    "    \n",
    "    trial_num = params[\"trial_num\"]\n",
    "    \n",
    "    if USE_MLFLOW:\n",
    "        mlflowclient = MlflowClient()\n",
    "        run = mlflowclient.create_run(experiment_id=\"9\", run_name=f\"run-{trial_num}\") #run_name=f\"run-{trial_num}\", nested=True): \n",
    "        \n",
    "        for param, value in params.items(): \n",
    "            mlflowclient.log_param(run.info.run_id, param, value) \n",
    "\n",
    "    params_copy = params.copy()\n",
    "    params_copy.pop(\"trial_num\") # remove trial_num as it is not a parameter for the BDT\n",
    "            \n",
    "    # initialize model with current parameters\n",
    "    model = XGBClassifier(random_state=5, booster='gbtree', **params) \n",
    "        \n",
    "    # fit model to training sample\n",
    "    model.fit(features_train, labels_train)\n",
    "    \n",
    "    # predictions using trained model\n",
    "    predict_train = model.predict(features_train)\n",
    "    predict_proba_train = model.predict_proba(features_train)[:, 1]\n",
    "    \n",
    "    # calculated jet accuracy for training sample\n",
    "    predict_proba_train_evt = predict_proba_train.reshape((int(len(predict_proba_train)/12),12))\n",
    "    predicted_combination_train = np.argmax(predict_proba_train_evt,axis=1)\n",
    "\n",
    "    scores = np.zeros(len(which_combination_train))\n",
    "    zipped = list(zip(which_combination_train.tolist(), predicted_combination_train.tolist()))\n",
    "    for i in range(len(which_combination_train)):\n",
    "        scores[i] = evaluation_matrix[zipped[i]]\n",
    "    jet_accuracy_train = -sum(scores)/len(scores)\n",
    "        \n",
    "    # log training metrics\n",
    "    if USE_MLFLOW:\n",
    "        mlflowclient.log_metric(run.info.run_id, 'train_accuracy', \n",
    "                                accuracy_score(labels_train, predict_train))\n",
    "        mlflowclient.log_metric(run.info.run_id, 'train_precision', \n",
    "                                precision_score(labels_train, predict_train, zero_division=0))\n",
    "        mlflowclient.log_metric(run.info.run_id, 'train_recall', \n",
    "                                recall_score(labels_train, predict_train))\n",
    "        mlflowclient.log_metric(run.info.run_id, 'train_f1', \n",
    "                                f1_score(labels_train, predict_train))\n",
    "        mlflowclient.log_metric(run.info.run_id, 'train_roc_auc', \n",
    "                                roc_auc_score(labels_train, predict_proba_train))\n",
    "        mlflowclient.log_metric(run.info.run_id, 'train_jet_accuracy', jet_accuracy_train)\n",
    "        \n",
    "    # predictions using trained model\n",
    "    predict_val= model.predict(features_val)\n",
    "    predict_proba_val = model.predict_proba(features_val)[:, 1]\n",
    "    \n",
    "    # calculated jet accuracy for validation sample\n",
    "    predict_proba_val_evt = predict_proba_val.reshape((int(len(predict_proba_val)/12),12))\n",
    "    predicted_combination_val = np.argmax(predict_proba_val_evt,axis=1)\n",
    "\n",
    "    scores = np.zeros(len(which_combination_val))\n",
    "    zipped = list(zip(which_combination_val.tolist(), predicted_combination_val.tolist()))\n",
    "    for i in range(len(which_combination_val)):\n",
    "        scores[i] = evaluation_matrix[zipped[i]]\n",
    "    jet_accuracy_val = -sum(scores)/len(scores)\n",
    "    \n",
    "    if USE_MLFLOW:\n",
    "        mlflowclient.log_metric(run.info.run_id, 'val_accuracy', \n",
    "                                accuracy_score(labels_val, predict_val))\n",
    "        mlflowclient.log_metric(run.info.run_id, 'val_precision', \n",
    "                                precision_score(labels_val, predict_val, zero_division=0))\n",
    "        mlflowclient.log_metric(run.info.run_id, 'val_recall', \n",
    "                                recall_score(labels_val, predict_val))\n",
    "        mlflowclient.log_metric(run.info.run_id, 'val_f1', \n",
    "                                f1_score(labels_val, predict_val))\n",
    "        mlflowclient.log_metric(run.info.run_id, 'val_roc_auc', \n",
    "                                roc_auc_score(labels_val, predict_proba_val))\n",
    "        mlflowclient.log_metric(run.info.run_id, 'val_jet_accuracy', jet_accuracy_val)\n",
    "    \n",
    "        # logging model\n",
    "        signature = infer_signature(features_train, predict_train)\n",
    "        # mlflow.xgboost.log_model(model, f'sigbkg_bdt_{trial_num}', signature=signature)\n",
    "\n",
    "        # explicitly close client\n",
    "        mlflowclient.set_terminated(run.info.run_id)\n",
    "        \n",
    "    return jet_accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719def11-124f-4d13-a2b1-f0064c1ada79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to transfer env. variables to workers\n",
    "def initialize_mlflow(): \n",
    "\n",
    "    os.environ['MLFLOW_TRACKING_URI'] = \"https://mlflow.software-dev.ncsa.cloud\"\n",
    "    os.environ['MLFLOW_S3_ENDPOINT_URL'] = \"https://mlflow-minio-api.software-dev.ncsa.cloud\"\n",
    "    os.environ['AWS_ACCESS_KEY_ID'] = \"\"\n",
    "    os.environ['AWS_SECRET_ACCESS_KEY'] = \"\"\n",
    "    \n",
    "    mlflow.set_tracking_uri('https://mlflow.software-dev.ncsa.cloud') \n",
    "    mlflow.set_experiment(\"agc-demo-example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24b9c73-26a2-4c39-baef-2d1faed85d71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c2281b-0613-4536-b935-8c59e3b60789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300fa078-91b1-497d-8ab4-dac98911a73b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d046d7f-30ac-4fb6-be06-677eb67b1367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43499e5e-681b-4ea0-a50c-a823f7b803d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_labels = np.array(labels_dict[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a6a970-ffef-4016-b9d3-595cdeb566d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_matrix = np.zeros((12,12))\n",
    "for i in range(len(permutation_labels)):\n",
    "    for j in range(len(permutation_labels)):\n",
    "        evaluation_matrix[i,j]=sum(np.equal(permutation_labels[i,:],permutation_labels[j,:]))/4\n",
    "print(evaluation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250f0e2f-775c-490d-882b-ddef22e88476",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"https://mlflow.software-dev.ncsa.cloud\")\n",
    "EXPERIMENT_ID = mlflow.set_experiment('optimize-reconstruction-bdt-00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92038c1-cc9c-43f5-894c-cfa1823af4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env MLFLOW_TRACKING_URI=https://mlflow.software-dev.ncsa.cloud\n",
    "%env MLFLOW_S3_ENDPOINT_URL=https://mlflow-minio-api.software-dev.ncsa.cloud\n",
    "%env AWS_ACCESS_KEY_ID=\n",
    "%env AWS_SECRET_ACCESS_KEY=leftfoot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f87f54-f866-412d-9e18-a1409f14a398",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_experiment=dict(mlflow.get_experiment_by_name('optimize-reconstruction-bdt-00'))\n",
    "EXP_ID=current_experiment['experiment_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97d2b19-de36-477c-b510-c8d94a46bcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training method for hyperopt\n",
    "def train_and_evaluate(params):\n",
    "    \n",
    "    # mlflow.xgboost.autolog()\n",
    "    \n",
    "    with mlflow.start_run(experiment_id=EXP_ID, nested=True):\n",
    "    \n",
    "        model = xgb.XGBClassifier(**params) # define model with current parameters\n",
    "        model = model.fit(features_train, labels_train) # train model\n",
    "        \n",
    "        mlflow.log_params(params)\n",
    "\n",
    "        # predicting train set and validation set\n",
    "        train_predicted = model.predict(features_train)\n",
    "        train_predicted_prob = model.predict_proba(features_train)[:, 1]\n",
    "        val_predicted = model.predict(features_val)\n",
    "        val_predicted_prob = model.predict_proba(features_val)[:, 1]\n",
    "\n",
    "        # model metrics to track\n",
    "        metric_names = ['accuracy', 'precision', 'recall', 'f1', 'aucroc']\n",
    "\n",
    "        # Training evaluation metrics\n",
    "        train_metrics = {\n",
    "            'Accuracy': accuracy_score(labels_train, train_predicted), \n",
    "            'Precision': precision_score(labels_train, train_predicted, zero_division=0), \n",
    "            'Recall': recall_score(labels_train, train_predicted), \n",
    "            'F1': f1_score(labels_train, train_predicted), \n",
    "            'AUCROC': roc_auc_score(labels_train, train_predicted_prob),\n",
    "        }\n",
    "\n",
    "        # Validation evaluation metrics\n",
    "        val_metrics = {\n",
    "            'Accuracy': accuracy_score(labels_val, val_predicted), \n",
    "            'Precision': precision_score(labels_val, val_predicted, zero_division=0), \n",
    "            'Recall': recall_score(labels_val, val_predicted), \n",
    "            'F1': f1_score(labels_val, val_predicted), \n",
    "            'AUCROC': roc_auc_score(labels_val, val_predicted_prob),\n",
    "        }\n",
    "\n",
    "        train_predicted_prob = train_predicted_prob.reshape((int(len(train_predicted_prob)/12),12))\n",
    "        train_predicted_combination = np.argmax(train_predicted_prob,axis=1)\n",
    "\n",
    "        scores = np.zeros(len(which_combination_train))\n",
    "        zipped = list(zip(which_combination_train.tolist(), train_predicted_combination.tolist()))\n",
    "        for i in range(len(which_combination_train)):\n",
    "            scores[i] = evaluation_matrix[zipped[i]]\n",
    "        score_train = -sum(scores)/len(scores)\n",
    "        \n",
    "        val_predicted_prob = val_predicted_prob.reshape((int(len(val_predicted_prob)/12),12))\n",
    "        val_predicted_combination = np.argmax(val_predicted_prob,axis=1)\n",
    "\n",
    "        scores = np.zeros(len(which_combination_val))\n",
    "        zipped = list(zip(which_combination_val.tolist(), val_predicted_combination.tolist()))\n",
    "        for i in range(len(which_combination_val)):\n",
    "            scores[i] = evaluation_matrix[zipped[i]]\n",
    "        score_val = -sum(scores)/len(scores)\n",
    "        \n",
    "        train_metrics[\"Jet-Accuracy\"] = -score_train\n",
    "        train_metrics_values = list(train_metrics.values())\n",
    "        \n",
    "        val_metrics[\"Jet-Accuracy\"] = -score_val\n",
    "        val_metrics_values = list(val_metrics.values())\n",
    "        \n",
    "        # Logging model signature, class, and name\n",
    "        signature = infer_signature(features_train, val_predicted)\n",
    "        mlflow.xgboost.log_model(model, 'model', signature=signature)\n",
    "        mlflow.set_tag('estimator_name', model.__class__.__name__)\n",
    "        mlflow.set_tag('estimator_class', model.__class__)\n",
    "\n",
    "        metric_names = ['accuracy', 'precision', 'recall', 'f1', 'aucroc', \"jet-accuracy\"]\n",
    "        # Logging each metric\n",
    "        for name, metric in list(zip(metric_names, train_metrics_values)):\n",
    "            mlflow.log_metric(f'training_{name}', metric)\n",
    "        for name, metric in list(zip(metric_names, val_metrics_values)):\n",
    "            mlflow.log_metric(f'validation_{name}', metric)\n",
    "\n",
    "    return {'status': STATUS_OK, 'loss': score_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1384f994-2c11-4886-8e7f-860ff313df7e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "\n",
    "# optimize model\n",
    "with mlflow.start_run(experiment_id=EXP_ID, run_name='xgboost_bdt_models'):\n",
    "    best_parameters = fmin(\n",
    "        fn=train_and_evaluate, \n",
    "        space=trial_params,\n",
    "        algo=tpe.suggest,\n",
    "        trials=trials,\n",
    "        max_evals=20 # how many trials to run\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62e26ae-6694-4586-9404-7ab0f6ec79b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert max_depth and n_estimators to integers\n",
    "best_parameters['max_depth'] = int(best_parameters['max_depth'])\n",
    "best_parameters['n_estimators'] = int(best_parameters['n_estimators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a6c3da-4d01-4616-af5b-9901af10cda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Model Parameters = \", best_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080b3617-cc4a-4598-b86b-95796f3ee71d",
   "metadata": {},
   "source": [
    "# Training/Evaluation with Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc184ef0-d7ee-457a-bd96-568b4487a85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and fit model to data\n",
    "model = xgb.XGBClassifier(**best_parameters)\n",
    "model = model.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf68c830-e234-4695-a5e1-1b62ec31ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "train_predicted = model.predict(features_train)\n",
    "train_predicted_prob = model.predict_proba(features_train)[:, 1]\n",
    "val_predicted = model.predict(features_val)\n",
    "val_predicted_prob = model.predict_proba(features_val)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9197c616-e84d-4818-82d8-06def49c82c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = accuracy_score(labels_train, train_predicted).round(3)\n",
    "train_precision = precision_score(labels_train, train_predicted).round(3)\n",
    "train_recall = recall_score(labels_train, train_predicted).round(3)\n",
    "train_f1 = f1_score(labels_train, train_predicted).round(3)\n",
    "train_aucroc = roc_auc_score(labels_train, train_predicted_prob).round(3)\n",
    "print(\"Training Accuracy = \", train_accuracy)\n",
    "print(\"Training Precision = \", train_precision)\n",
    "print(\"Training Recall = \", train_recall)\n",
    "print(\"Training f1 = \", train_f1)\n",
    "print(\"Training AUC = \", train_aucroc)\n",
    "print()\n",
    "\n",
    "val_accuracy = accuracy_score(labels_val, val_predicted).round(3)\n",
    "val_precision = precision_score(labels_val, val_predicted).round(3)\n",
    "val_recall = recall_score(labels_val, val_predicted).round(3)\n",
    "val_f1 = f1_score(labels_val, val_predicted).round(3)\n",
    "val_aucroc = roc_auc_score(labels_val, val_predicted_prob).round(3)\n",
    "print(\"Validation Accuracy = \", val_accuracy)\n",
    "print(\"Validation Precision = \", val_precision)\n",
    "print(\"Validation Recall = \", val_recall)\n",
    "print(\"Validation f1 = \", val_f1)\n",
    "print(\"Validation AUC = \", val_aucroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19afd15-1e6c-45b2-b2ba-35634e00ebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predicted_prob = val_predicted_prob.reshape((int(len(val_predicted_prob)/12),12))\n",
    "val_predicted_combination = np.argmax(val_predicted_prob,axis=1)\n",
    "    \n",
    "scores = np.zeros(len(which_combination_val))\n",
    "zipped = list(zip(which_combination_val.tolist(), val_predicted_combination.tolist()))\n",
    "for i in range(len(which_combination_val)):\n",
    "    scores[i] = evaluation_matrix[zipped[i]]\n",
    "        \n",
    "score = -sum(scores)/len(scores)\n",
    "print(\"Validation Jet Score = \", score)\n",
    "\n",
    "train_predicted_prob = train_predicted_prob.reshape((int(len(train_predicted_prob)/12),12))\n",
    "train_predicted_combination = np.argmax(train_predicted_prob,axis=1)\n",
    "    \n",
    "scores = np.zeros(len(which_combination_train))\n",
    "zipped = list(zip(which_combination_train.tolist(), train_predicted_combination.tolist()))\n",
    "for i in range(len(which_combination_train)):\n",
    "    scores[i] = evaluation_matrix[zipped[i]]\n",
    "        \n",
    "score = -sum(scores)/len(scores)\n",
    "print(\"Training Jet Score = \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdbe3b9-8f2a-4dd8-836a-282c7ba06916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "test_predicted = model.predict(features_test)\n",
    "test_predicted_prob = model.predict_proba(features_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045ba47c-fa88-486e-a2ae-c2e2d4b3af3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = accuracy_score(labels_test, test_predicted).round(3)\n",
    "test_precision = precision_score(labels_test, test_predicted).round(3)\n",
    "test_recall = recall_score(labels_test, test_predicted).round(3)\n",
    "test_f1 = f1_score(labels_test, test_predicted).round(3)\n",
    "test_aucroc = roc_auc_score(labels_test, test_predicted_prob).round(3)\n",
    "print(\"Test Accuracy = \", test_accuracy)\n",
    "print(\"Test Precision = \", test_precision)\n",
    "print(\"Test Recall = \", test_recall)\n",
    "print(\"Test f1 = \", test_f1)\n",
    "print(\"Test AUC = \", test_aucroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284704ff-586a-4a04-bedd-188371224502",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted_prob = test_predicted_prob.reshape((int(len(test_predicted_prob)/12),12))\n",
    "test_predicted_combination = np.argmax(test_predicted_prob,axis=1)\n",
    "    \n",
    "scores = np.zeros(len(which_combination_test))\n",
    "zipped = list(zip(which_combination_test.tolist(), test_predicted_combination.tolist()))\n",
    "for i in range(len(which_combination_test)):\n",
    "    scores[i] = evaluation_matrix[zipped[i]]\n",
    "        \n",
    "score = sum(scores)/len(scores)\n",
    "print(\"Test Jet Score = \", score)\n",
    "print(\"Random Assignment Jet Score = \", 0.375)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aebb93-2ffb-4528-881d-ecc8cff8219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"How many events are 100% correct: \", sum(scores==1)/len(scores), \", Random = \",sum(evaluation_matrix[0,:]==1)/12)\n",
    "print(\"How many events are 50% correct: \", sum(scores==0.5)/len(scores), \", Random = \",sum(evaluation_matrix[0,:]==0.5)/12)\n",
    "print(\"How many events are 25% correct: \", sum(scores==0.25)/len(scores), \", Random = \",sum(evaluation_matrix[0,:]==0.25)/12)\n",
    "print(\"How many events are 0% correct: \", sum(scores==0)/len(scores), \", Random = \",sum(evaluation_matrix[0,:]==0)/12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3479993-ba23-4e56-aa08-d5a449916c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to json. this file can be used with the FIL backend in nvidia-triton!\n",
    "model.save_model(\"models/model_xgb_230206.json\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
