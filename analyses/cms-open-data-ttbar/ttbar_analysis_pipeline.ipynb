{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dc37683",
   "metadata": {},
   "source": [
    "# CMS Open Data $t\\bar{t}$: from data delivery to statistical inference\n",
    "\n",
    "We are using [2015 CMS Open Data](https://cms.cern/news/first-cms-open-data-lhc-run-2-released) in this demonstration to showcase an analysis pipeline.\n",
    "It features data delivery and processing, histogram construction and visualization, as well as statistical inference.\n",
    "\n",
    "This notebook was developed in the context of the [IRIS-HEP AGC tools 2022 workshop](https://indico.cern.ch/e/agc-tools-2).\n",
    "This work was supported by the U.S. National Science Foundation (NSF) Cooperative Agreement OAC-1836650 (IRIS-HEP).\n",
    "\n",
    "This is a **technical demonstration**.\n",
    "We are including the relevant workflow aspects that physicists need in their work, but we are not focusing on making every piece of the demonstration physically meaningful.\n",
    "This concerns in particular systematic uncertainties: we capture the workflow, but the actual implementations are more complex in practice.\n",
    "If you are interested in the physics side of analyzing top pair production, check out the latest results from [ATLAS](https://twiki.cern.ch/twiki/bin/view/AtlasPublic/TopPublicResults) and [CMS](https://cms-results.web.cern.ch/cms-results/public-results/preliminary-results/)!\n",
    "If you would like to see more technical demonstrations, also check out an [ATLAS Open Data example](https://indico.cern.ch/event/1076231/contributions/4560405/) demonstrated previously.\n",
    "\n",
    "This notebook implements most of the analysis pipeline shown in the following picture, using the tools also mentioned there:\n",
    "![ecosystem visualization](utils/ecosystem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404927fe",
   "metadata": {},
   "source": [
    "### Data pipelines\n",
    "\n",
    "There are two possible pipelines: one with `ServiceX` enabled, and one using only `coffea` for processing.\n",
    "![processing pipelines](utils/processing_pipelines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd72d323",
   "metadata": {},
   "source": [
    "### Imports: setting up our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16b3b058",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import awkward as ak\n",
    "import cabinetry\n",
    "import correctionlib\n",
    "from coffea import processor\n",
    "from coffea.nanoevents import NanoAODSchema\n",
    "from coffea.analysis_tools import PackedSelection\n",
    "from func_adl import ObjectStream\n",
    "from func_adl_servicex import ServiceXSourceUpROOT\n",
    "import hist\n",
    "import json\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import uproot\n",
    "from xgboost import XGBClassifier\n",
    "import pyhf\n",
    "\n",
    "import utils  # contains code for bookkeeping and cosmetics, as well as some boilerplate\n",
    "\n",
    "logging.getLogger(\"cabinetry\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd573bb1",
   "metadata": {},
   "source": [
    "### Configuration: number of files and data delivery path\n",
    "\n",
    "The number of files per sample set here determines the size of the dataset we are processing. There are 9 samples being used here, all part of the 2015 CMS Open Data release.\n",
    "\n",
    "These samples were originally published in miniAOD format, but for the purposes of this demonstration were pre-converted into nanoAOD format. More details about the inputs can be found [here](https://github.com/iris-hep/analysis-grand-challenge/tree/main/datasets/cms-open-data-2015).\n",
    "\n",
    "The table below summarizes the amount of data processed depending on the `N_FILES_MAX_PER_SAMPLE` setting.\n",
    "\n",
    "| setting | number of files | total size | number of events |\n",
    "| --- | --- | --- | --- |\n",
    "| `1` | 9 | 22.9 GB | 10455719 |\n",
    "| `2` | 18 | 42.8 GB | 19497435 |\n",
    "| `5` | 43 | 105 GB | 47996231 |\n",
    "| `10` | 79 | 200 GB | 90546458 |\n",
    "| `20` | 140 | 359 GB | 163123242 |\n",
    "| `50` | 255 | 631 GB | 297247463 |\n",
    "| `100` | 395 | 960 GB | 470397795 |\n",
    "| `200` | 595 | 1.40 TB | 705273291 |\n",
    "| `-1` | 787 | 1.78 TB | 940160174 |\n",
    "\n",
    "The input files are all in the 1â€“3 GB range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e43e9b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### GLOBAL CONFIGURATION\n",
    "# input files per process, set to e.g. 10 (smaller number = faster)\n",
    "N_FILES_MAX_PER_SAMPLE = 1\n",
    "\n",
    "# enable Dask\n",
    "USE_DASK = True\n",
    "\n",
    "# enable ServiceX\n",
    "USE_SERVICEX = True\n",
    "\n",
    "### LOAD OTHER CONFIGURATION VARIABLES\n",
    "with open(\"config.yaml\") as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "### ML-INFERENCE SETTINGS\n",
    "\n",
    "# enable ML inference\n",
    "USE_INFERENCE = True\n",
    "\n",
    "# enable inference using NVIDIA Triton server\n",
    "USE_TRITON = False\n",
    "\n",
    "### LOAD OTHER CONFIGURATION VARIABLES\n",
    "with open(\"config.yaml\") as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "config[\"ml\"][\"USE_INFERENCE\"] = USE_INFERENCE\n",
    "config[\"ml\"][\"USE_TRITON\"] = USE_TRITON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea79d2d",
   "metadata": {},
   "source": [
    "### Machine Learning Task\n",
    "\n",
    "During the processing step, machine learning is used to calculate one of the variables used for this analysis. The models used are trained separately in the `jetassignment_training.ipynb` notebook. Jets in the events are assigned to labels corresponding with their parent partons using a boosted decision tree (BDT). More information about the model and training can be found within that notebook. To obtain the features used as inputs for the BDT, we use the methods defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dd8003a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of permutations for n= 4 :  12\n",
      "number of permutations for n= 5 :  60\n",
      "number of permutations for n= 6 :  180\n"
     ]
    }
   ],
   "source": [
    "permutations_dict = utils.get_permutations_dict(config[\"ml\"][\"MAX_N_JETS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59eb3658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_features(jets, electrons, muons, permutations_dict):\n",
    "    '''\n",
    "    Calculate features for each of the 12 combinations per event\n",
    "\n",
    "    Args:\n",
    "        jets: selected jets\n",
    "        electrons: selected electrons\n",
    "        muons: selected muons\n",
    "        permutations_dict: which permutations to consider for each number of jets in an event\n",
    "\n",
    "    Returns:\n",
    "        features (flattened to remove event level)\n",
    "        perm_counts: how many permutations in each event. use to unflatten features\n",
    "    '''\n",
    "\n",
    "    # calculate number of jets in each event\n",
    "    njet = ak.num(jets).to_numpy()\n",
    "    # don't consider every jet for events with high jet multiplicity\n",
    "    njet[njet>max(permutations_dict.keys())] = max(permutations_dict.keys())\n",
    "    # create awkward array of permutation indices\n",
    "    perms = ak.Array([permutations_dict[n] for n in njet])\n",
    "    perm_counts = ak.num(perms)\n",
    "\n",
    "    #### calculate features ####\n",
    "    features = np.zeros((sum(perm_counts),20))\n",
    "\n",
    "    # grab lepton info\n",
    "    leptons = ak.flatten(ak.concatenate((electrons, muons),axis=1),axis=-1)\n",
    "\n",
    "    feature_count = 0\n",
    "\n",
    "    # delta R between b_toplep and lepton\n",
    "    features[:,0] = ak.flatten(np.sqrt((leptons.eta - jets[perms[...,3]].eta)**2 +\n",
    "                                       (leptons.phi - jets[perms[...,3]].phi)**2)).to_numpy()\n",
    "\n",
    "\n",
    "    #delta R between the two W\n",
    "    features[:,1] = ak.flatten(np.sqrt((jets[perms[...,0]].eta - jets[perms[...,1]].eta)**2 +\n",
    "                                       (jets[perms[...,0]].phi - jets[perms[...,1]].phi)**2)).to_numpy()\n",
    "\n",
    "    #delta R between W and b_tophad\n",
    "    features[:,2] = ak.flatten(np.sqrt((jets[perms[...,0]].eta - jets[perms[...,2]].eta)**2 +\n",
    "                                       (jets[perms[...,0]].phi - jets[perms[...,2]].phi)**2)).to_numpy()\n",
    "    features[:,3] = ak.flatten(np.sqrt((jets[perms[...,1]].eta - jets[perms[...,2]].eta)**2 +\n",
    "                                       (jets[perms[...,1]].phi - jets[perms[...,2]].phi)**2)).to_numpy()\n",
    "\n",
    "    # combined mass of b_toplep and lepton\n",
    "    features[:,4] = ak.flatten((leptons + jets[perms[...,3]]).mass).to_numpy()\n",
    "\n",
    "    # combined mass of W\n",
    "    features[:,5] = ak.flatten((jets[perms[...,0]] + jets[perms[...,1]]).mass).to_numpy()\n",
    "\n",
    "    # combined mass of W and b_tophad\n",
    "    features[:,6] = ak.flatten((jets[perms[...,0]] + jets[perms[...,1]] +\n",
    "                                 jets[perms[...,2]]).mass).to_numpy()\n",
    "\n",
    "    feature_count+=1\n",
    "    # combined pT of W and b_tophad\n",
    "    features[:,7] = ak.flatten((jets[perms[...,0]] + jets[perms[...,1]] +\n",
    "                                 jets[perms[...,2]]).pt).to_numpy()\n",
    "\n",
    "\n",
    "    # pt of every jet\n",
    "    features[:,8] = ak.flatten(jets[perms[...,0]].pt).to_numpy()\n",
    "    features[:,9] = ak.flatten(jets[perms[...,1]].pt).to_numpy()\n",
    "    features[:,10] = ak.flatten(jets[perms[...,2]].pt).to_numpy()\n",
    "    features[:,11] = ak.flatten(jets[perms[...,3]].pt).to_numpy()\n",
    "\n",
    "    # btagCSVV2 of every jet\n",
    "    features[:,12] = ak.flatten(jets[perms[...,0]].btagCSVV2).to_numpy()\n",
    "    features[:,13] = ak.flatten(jets[perms[...,1]].btagCSVV2).to_numpy()\n",
    "    features[:,14] = ak.flatten(jets[perms[...,2]].btagCSVV2).to_numpy()\n",
    "    features[:,15] = ak.flatten(jets[perms[...,3]].btagCSVV2).to_numpy()\n",
    "\n",
    "    # quark-gluon likelihood discriminator of every jet\n",
    "    features[:,16] = ak.flatten(jets[perms[...,0]].qgl).to_numpy()\n",
    "    features[:,17] = ak.flatten(jets[perms[...,1]].qgl).to_numpy()\n",
    "    features[:,18] = ak.flatten(jets[perms[...,2]].qgl).to_numpy()\n",
    "    features[:,19] = ak.flatten(jets[perms[...,3]].qgl).to_numpy()\n",
    "\n",
    "    return features, perm_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141d6520",
   "metadata": {},
   "source": [
    "### Defining our `coffea` Processor\n",
    "\n",
    "The processor includes a lot of the physics analysis details:\n",
    "- event filtering and the calculation of observables,\n",
    "- event weighting,\n",
    "- calculating systematic uncertainties at the event and object level,\n",
    "- filling all the information into histograms that get aggregated and ultimately returned to us by `coffea`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ee2b3a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# functions creating systematic variations\n",
    "def jet_pt_resolution(pt):\n",
    "    # normal distribution with 5% variations, shape matches jets\n",
    "    counts = ak.num(pt)\n",
    "    pt_flat = ak.flatten(pt)\n",
    "    resolution_variation = np.random.normal(np.ones_like(pt_flat), 0.05)\n",
    "    return ak.unflatten(resolution_variation, counts)\n",
    "\n",
    "class TtbarAnalysis(processor.ProcessorABC):\n",
    "    def __init__(self,\n",
    "                 use_dask,\n",
    "                 disable_processing,\n",
    "                 io_branches,\n",
    "                 ml_options,\n",
    "                 xgboost_model_even,\n",
    "                 xgboost_model_odd,\n",
    "                 permutations_dict={}):\n",
    "\n",
    "        num_bins = 25\n",
    "        bin_low = 50\n",
    "        bin_high = 550\n",
    "        name = \"observable\"\n",
    "        label = \"observable [GeV]\"\n",
    "        self.hist = (\n",
    "            hist.Hist.new.Reg(num_bins, bin_low, bin_high, name=name, label=label)\n",
    "            .StrCat([\"4j1b\", \"4j2b\"], name=\"region\", label=\"Region\")\n",
    "            .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "            .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "            .Weight()\n",
    "        )\n",
    "\n",
    "        self.use_dask = use_dask\n",
    "        self.disable_processing = disable_processing\n",
    "        self.io_branches = io_branches\n",
    "        self.cset = correctionlib.CorrectionSet.from_file(\"corrections.json\")\n",
    "\n",
    "        self.use_inference = ml_options[\"USE_INFERENCE\"]\n",
    "        if self.use_inference:\n",
    "            self.ml_hist_dict = {}\n",
    "            self.feature_names = ml_options[\"FEATURE_NAMES\"]\n",
    "            feature_descriptions = ml_options[\"FEATURE_DESCRIPTIONS\"]\n",
    "            for i in range(len(self.feature_names)):\n",
    "                self.ml_hist_dict[f\"hist_{self.feature_names[i]}\"] = (\n",
    "                    hist.Hist.new.Reg(num_bins,\n",
    "                                      ml_options[\"BIN_RANGES\"][i][0],\n",
    "                                      ml_options[\"BIN_RANGES\"][i][1],\n",
    "                                      name=\"observable\",\n",
    "                                      label=feature_descriptions[i])\n",
    "                    .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "                    .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "                    .Weight()\n",
    "                )\n",
    "\n",
    "            # for ML inference\n",
    "            self.use_triton = ml_options[\"USE_TRITON\"]\n",
    "            self.xgboost_model_even = xgboost_model_even\n",
    "            self.xgboost_model_odd = xgboost_model_odd\n",
    "            self.model_name = ml_options[\"MODEL_NAME\"]\n",
    "            self.model_vers_even = ml_options[\"MODEL_VERSION_EVEN\"]\n",
    "            self.model_vers_odd = ml_options[\"MODEL_VERSION_ODD\"]\n",
    "            self.url = ml_options[\"TRITON_URL\"]\n",
    "            self.permutations_dict = permutations_dict\n",
    "\n",
    "    def only_do_IO(self, events):\n",
    "        for branch in self.io_branches:\n",
    "            if \"_\" in branch:\n",
    "                split = branch.split(\"_\")\n",
    "                object_type = split[0]\n",
    "                property_name = '_'.join(split[1:])\n",
    "                ak.materialized(events[object_type][property_name])\n",
    "            else:\n",
    "                ak.materialized(events[branch])\n",
    "        return {\"hist\": {}}\n",
    "\n",
    "    def process(self, events):\n",
    "        if self.disable_processing:\n",
    "            # IO testing with no subsequent processing\n",
    "            return self.only_do_IO(events)\n",
    "\n",
    "        histogram = self.hist.copy()\n",
    "        if self.use_inference:\n",
    "            ml_hist_dict = {}\n",
    "            for i in range(len(self.feature_names)):\n",
    "                ml_hist_dict[f\"hist_{self.feature_names[i]}\"] = self.ml_hist_dict[f\"hist_{self.feature_names[i]}\"].copy()\n",
    "\n",
    "        process = events.metadata[\"process\"]  # \"ttbar\" etc.\n",
    "        variation = events.metadata[\"variation\"]  # \"nominal\" etc.\n",
    "\n",
    "        # normalization for MC\n",
    "        x_sec = events.metadata[\"xsec\"]\n",
    "        nevts_total = events.metadata[\"nevts\"]\n",
    "        lumi = 3378 # /pb\n",
    "        if process != \"data\":\n",
    "            xsec_weight = x_sec * lumi / nevts_total\n",
    "        else:\n",
    "            xsec_weight = 1\n",
    "\n",
    "        # setup triton gRPC client\n",
    "        if self.use_inference:\n",
    "            if self.use_triton:\n",
    "                import tritonclient.grpc as grpcclient\n",
    "                triton_client = grpcclient.InferenceServerClient(url=self.url)\n",
    "                model_metadata = triton_client.get_model_metadata(self.model_name, self.model_vers_even)\n",
    "                input_name = model_metadata.inputs[0].name\n",
    "                dtype = model_metadata.inputs[0].datatype\n",
    "                output_name = model_metadata.outputs[0].name\n",
    "\n",
    "            elif not self.use_dask:\n",
    "                model_even = XGBClassifier()\n",
    "                model_even.load_model(self.xgboost_model_even)\n",
    "                self.xgboost_model_even = model_even\n",
    "\n",
    "                model_odd = XGBClassifier()\n",
    "                model_odd.load_model(self.xgboost_model_odd)\n",
    "                self.xgboost_model_odd = model_odd\n",
    "\n",
    "        #### systematics\n",
    "        # jet energy scale / resolution systematics\n",
    "        # need to adjust schema to instead use coffea add_systematic feature, especially for ServiceX\n",
    "        # cannot attach pT variations to events.jet, so attach to events directly\n",
    "        # and subsequently scale pT by these scale factors\n",
    "        events[\"pt_scale_up\"] = 1.03\n",
    "        events[\"pt_res_up\"] = jet_pt_resolution(events.Jet.pt)\n",
    "\n",
    "        syst_variations = [\"nominal\"]\n",
    "        jet_kinematic_systs = [\"pt_scale_up\", \"pt_res_up\"]\n",
    "        event_systs = [f\"btag_var_{i}\" for i in range(4)]\n",
    "        if process == \"wjets\":\n",
    "            event_systs.append(\"scale_var\")\n",
    "\n",
    "        # Only do systematics for nominal samples, e.g. ttbar__nominal\n",
    "        if variation == \"nominal\":\n",
    "            syst_variations.extend(jet_kinematic_systs)\n",
    "            syst_variations.extend(event_systs)\n",
    "\n",
    "        # for pt_var in pt_variations:\n",
    "        for syst_var in syst_variations:\n",
    "            ### event selection\n",
    "            # very very loosely based on https://arxiv.org/abs/2006.13076\n",
    "\n",
    "            # Note: This creates new objects, distinct from those in the 'events' object\n",
    "            elecs = events.Electron\n",
    "            muons = events.Muon\n",
    "            jets = events.Jet\n",
    "            if syst_var in jet_kinematic_systs:\n",
    "                # Replace jet.pt with the adjusted values\n",
    "                jets[\"pt\"] = jets.pt * events[syst_var]\n",
    "\n",
    "            electron_reqs = (elecs.pt > 30) & (np.abs(elecs.eta) < 2.1) & (elecs.cutBased == 4) & (elecs.sip3d < 4)\n",
    "            muon_reqs = ((muons.pt > 30) & (np.abs(muons.eta) < 2.1) & (muons.tightId) & (muons.sip3d < 4) &\n",
    "                         (muons.pfRelIso04_all < 0.15))\n",
    "            jet_reqs = (jets.pt > 30) & (np.abs(jets.eta) < 2.4) & (jets.isTightLeptonVeto)\n",
    "\n",
    "            # Only keep objects that pass our requirements\n",
    "            elecs = elecs[electron_reqs]\n",
    "            muons = muons[muon_reqs]\n",
    "            jets = jets[jet_reqs]\n",
    "\n",
    "            even = (events.event%2==0)  # whether events are even/odd\n",
    "\n",
    "            B_TAG_THRESHOLD = 0.5\n",
    "\n",
    "            ######### Store boolean masks with PackedSelection ##########\n",
    "            selections = PackedSelection(dtype='uint64')\n",
    "            # Basic selection criteria\n",
    "            selections.add(\"exactly_1l\", (ak.num(elecs) + ak.num(muons)) == 1)\n",
    "            selections.add(\"atleast_4j\", ak.num(jets) >= 4)\n",
    "            selections.add(\"exactly_1b\", ak.sum(jets.btagCSVV2 >= B_TAG_THRESHOLD, axis=1) == 1)\n",
    "            selections.add(\"atleast_2b\", ak.sum(jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 2)\n",
    "            # Complex selection criteria\n",
    "            selections.add(\"4j1b\", selections.all(\"exactly_1l\", \"atleast_4j\", \"exactly_1b\"))\n",
    "            selections.add(\"4j2b\", selections.all(\"exactly_1l\", \"atleast_4j\", \"atleast_2b\"))\n",
    "\n",
    "            for region in [\"4j1b\", \"4j2b\"]:\n",
    "                region_selection = selections.all(region)\n",
    "                region_jets = jets[region_selection]\n",
    "                region_elecs = elecs[region_selection]\n",
    "                region_muons = muons[region_selection]\n",
    "                region_even = even[region_selection]\n",
    "                region_weights = np.ones(len(region_jets)) * xsec_weight\n",
    "\n",
    "                if region == \"4j1b\":\n",
    "                    observable = ak.sum(region_jets.pt, axis=-1)\n",
    "\n",
    "                elif region == \"4j2b\":\n",
    "\n",
    "                    # reconstruct hadronic top as bjj system with largest pT\n",
    "                    trijet = ak.combinations(region_jets, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidates\n",
    "                    trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # calculate four-momentum of tri-jet system\n",
    "                    trijet[\"max_btag\"] = np.maximum(trijet.j1.btagCSVV2, np.maximum(trijet.j2.btagCSVV2, trijet.j3.btagCSVV2))\n",
    "                    trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # at least one-btag in trijet candidates\n",
    "                    # pick trijet candidate with largest pT and calculate mass of system\n",
    "                    trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "                    observable = ak.flatten(trijet_mass)\n",
    "\n",
    "                    if sum(region_selection)==0: continue\n",
    "\n",
    "                    if self.use_inference:\n",
    "                        features, perm_counts = get_features(region_jets, region_elecs, region_muons, self.permutations_dict)\n",
    "                        even_perm = np.repeat(region_even, perm_counts)\n",
    "\n",
    "                        #calculate ml observable\n",
    "                        if self.use_triton:\n",
    "\n",
    "                            results = np.zeros(features.shape[0])\n",
    "                            output = grpcclient.InferRequestedOutput(output_name)\n",
    "\n",
    "                            if len(features[even_perm])>0:\n",
    "                                inpt = [grpcclient.InferInput(input_name, features[even_perm].shape, dtype)]\n",
    "                                inpt[0].set_data_from_numpy(features[even_perm].astype(np.float32))\n",
    "                                results[even_perm]=triton_client.infer(\n",
    "                                    model_name=self.model_name,\n",
    "                                    model_version=self.model_vers_even,\n",
    "                                    inputs=inpt,\n",
    "                                    outputs=[output]\n",
    "                                ).as_numpy(output_name)[:, 1]\n",
    "                            if len(features[np.invert(even_perm)])>0:\n",
    "                                inpt = [grpcclient.InferInput(input_name, features[np.invert(even_perm)].shape, dtype)]\n",
    "                                inpt[0].set_data_from_numpy(features[np.invert(even_perm)].astype(np.float32))\n",
    "                                results[np.invert(even_perm)]=triton_client.infer(\n",
    "                                    model_name=self.model_name,\n",
    "                                    model_version=self.model_vers_odd,\n",
    "                                    inputs=inpt,\n",
    "                                    outputs=[output]\n",
    "                                ).as_numpy(output_name)[:, 1]\n",
    "\n",
    "                        else:\n",
    "                            results = np.zeros(features.shape[0])\n",
    "                            if len(features[even_perm])>0:\n",
    "                                results[even_perm] = self.xgboost_model_odd.predict_proba(\n",
    "                                    features[even_perm,:])[:, 1]\n",
    "                            if len(features[np.invert(even_perm)])>0:\n",
    "                                results[np.invert(even_perm)] = results_odd = self.xgboost_model_even.predict_proba(\n",
    "                                    features[np.invert(even_perm),:])[:, 1]\n",
    "\n",
    "                        results = ak.unflatten(results, perm_counts)\n",
    "                        features = ak.flatten(ak.unflatten(features, perm_counts)[\n",
    "                            ak.from_regular(ak.argmax(results,axis=1)[:, np.newaxis])\n",
    "                        ])\n",
    "                syst_var_name = f\"{syst_var}\"\n",
    "                # Break up the filling into event weight systematics and object variation systematics\n",
    "                if syst_var in event_systs:\n",
    "                    for i_dir, direction in enumerate([\"up\", \"down\"]):\n",
    "                        # Should be an event weight systematic with an up/down variation\n",
    "                        if syst_var.startswith(\"btag_var\"):\n",
    "                            i_jet = int(syst_var.rsplit(\"_\",1)[-1])   # Kind of fragile\n",
    "                            wgt_variation = self.cset[\"event_systematics\"].evaluate(\"btag_var\", direction, region_jets.pt[:,i_jet])\n",
    "                        elif syst_var == \"scale_var\":\n",
    "                            # The pt array is only used to make sure the output array has the correct shape\n",
    "                            wgt_variation = self.cset[\"event_systematics\"].evaluate(\"scale_var\", direction, region_jets.pt[:,0])\n",
    "                        syst_var_name = f\"{syst_var}_{direction}\"\n",
    "                        histogram.fill(\n",
    "                            observable=observable, region=region, process=process,\n",
    "                            variation=syst_var_name, weight=region_weights * wgt_variation\n",
    "                        )\n",
    "                        if region==\"4j2b\" and self.use_inference:\n",
    "                            for i in range(len(self.feature_names)):\n",
    "                                ml_hist_dict[f\"hist_{self.feature_names[i]}\"].fill(observable=features[...,i],\n",
    "                                                                                   process=process,\n",
    "                                                                                   variation=syst_var_name,\n",
    "                                                                                   weight=region_weights * wgt_variation)\n",
    "                else:\n",
    "                    # Should either be 'nominal' or an object variation systematic\n",
    "                    if variation != \"nominal\":\n",
    "                        # This is a 2-point systematic, e.g. ttbar__scaledown, ttbar__ME_var, etc.\n",
    "                        syst_var_name = variation\n",
    "                    histogram.fill(\n",
    "                        observable=observable, region=region, process=process,\n",
    "                        variation=syst_var_name, weight=region_weights\n",
    "                    )\n",
    "                    if region==\"4j2b\" and self.use_inference:\n",
    "                        for i in range(len(self.feature_names)):\n",
    "                            ml_hist_dict[f\"hist_{self.feature_names[i]}\"].fill(observable=features[...,i],\n",
    "                                                                               process=process,\n",
    "                                                                               variation=syst_var_name,\n",
    "                                                                               weight=region_weights)\n",
    "\n",
    "\n",
    "        output = {\"nevents\": {events.metadata[\"dataset\"]: len(events)}, \"hist\": histogram}\n",
    "        if self.use_inference:\n",
    "            output[\"ml_hist_dict\"] = ml_hist_dict\n",
    "\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd4c9e",
   "metadata": {},
   "source": [
    "### \"Fileset\" construction and metadata\n",
    "\n",
    "Here, we gather all the required information about the files we want to process: paths to the files and asociated metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cf166ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processes in fileset: ['ttbar__nominal', 'ttbar__scaledown', 'ttbar__scaleup', 'ttbar__ME_var', 'ttbar__PS_var', 'single_top_s_chan__nominal', 'single_top_t_chan__nominal', 'single_top_tW__nominal', 'wjets__nominal']\n",
      "\n",
      "example of information in fileset:\n",
      "{\n",
      "  'files': [https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_00000_0000.root, ...],\n",
      "  'metadata': {'process': 'ttbar', 'variation': 'nominal', 'nevts': 1334428, 'xsec': 729.84}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "fileset = utils.construct_fileset(N_FILES_MAX_PER_SAMPLE, use_xcache=False, af_name=config[\"benchmarking\"][\"AF_NAME\"])  # local files on /data for ssl-dev\n",
    "\n",
    "print(f\"processes in fileset: {list(fileset.keys())}\")\n",
    "print(f\"\\nexample of information in fileset:\\n{{\\n  'files': [{fileset['ttbar__nominal']['files'][0]}, ...],\")\n",
    "print(f\"  'metadata': {fileset['ttbar__nominal']['metadata']}\\n}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b910d3d5",
   "metadata": {},
   "source": [
    "### ServiceX-specific functionality: query setup\n",
    "\n",
    "Define the func_adl query to be used for the purpose of extracting columns and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a032a148",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_query(source: ObjectStream) -> ObjectStream:\n",
    "    \"\"\"Query for event / column selection: >=4j >=1b, ==1 lep with pT>30 GeV + additional cuts, return relevant columns\n",
    "    \"\"\"\n",
    "    return source.Where(lambda e: {\"pt\": e.Electron_pt, \n",
    "                               \"eta\": e.Electron_eta, \n",
    "                               \"cutBased\": e.Electron_cutBased, \n",
    "                               \"sip3d\": e.Electron_sip3d,}.Zip()\\\n",
    "                    .Where(lambda electron: (electron.pt > 30 \n",
    "                                            and abs(electron.eta) < 2.1 \n",
    "                                            and electron.cutBased == 4 \n",
    "                                            and electron.sip3d < 4)).Count() \n",
    "                    + {\"pt\": e.Muon_pt, \n",
    "                       \"eta\": e.Muon_eta,\n",
    "                       \"tightId\": e.Muon_tightId,\n",
    "                       \"sip3d\": e.Muon_sip3d,\n",
    "                       \"pfRelIso04_all\": e.Muon_pfRelIso04_all}.Zip()\\\n",
    "                    .Where(lambda muon: (muon.pt > 30 \n",
    "                                         and abs(muon.eta) < 2.1 \n",
    "                                         and muon.tightId \n",
    "                                         and muon.pfRelIso04_all < 0.15)).Count()== 1)\\\n",
    "                    .Where(lambda f: {\"pt\": f.Jet_pt, \n",
    "                                      \"eta\": f.Jet_eta,\n",
    "                                      \"jetId\": f.Jet_jetId}.Zip()\\\n",
    "                           .Where(lambda jet: (jet.pt > 30 \n",
    "                                               and abs(jet.eta) < 2.4 \n",
    "                                               and jet.jetId == 6)).Count() >= 4)\\\n",
    "                 .Where(lambda g: {\"pt\": g.Jet_pt, \n",
    "                                   \"eta\": g.Jet_eta,\n",
    "                                   \"btagCSVV2\": g.Jet_btagCSVV2,\n",
    "                                   \"jetId\": g.Jet_jetId}.Zip()\\\n",
    "                        .Where(lambda jet: (jet.btagCSVV2 >= 0.5 \n",
    "                                            and jet.pt > 30\n",
    "                                            and abs(jet.eta) < 2.4) \n",
    "                                            and jet.jetId == 6).Count() >= 1)\\\n",
    "                    .Select(lambda h: {\"Electron_pt\": h.Electron_pt,\n",
    "                                       \"Electron_eta\": h.Electron_eta,\n",
    "                                       \"Electron_phi\": h.Electron_phi,\n",
    "                                       \"Electron_mass\": h.Electron_mass,\n",
    "                                       \"Electron_cutBased\": h.Electron_cutBased,\n",
    "                                       \"Electron_sip3d\": h.Electron_sip3d,\n",
    "                                       \"Muon_pt\": h.Muon_pt,\n",
    "                                       \"Muon_eta\": h.Muon_eta,\n",
    "                                       \"Muon_phi\": h.Muon_phi,\n",
    "                                       \"Muon_mass\": h.Muon_mass,\n",
    "                                       \"Muon_tightId\": h.Muon_tightId,\n",
    "                                       \"Muon_sip3d\": h.Muon_sip3d,\n",
    "                                       \"Muon_pfRelIso04_all\": h.Muon_pfRelIso04_all,\n",
    "                                       \"Jet_mass\": h.Jet_mass,\n",
    "                                       \"Jet_pt\": h.Jet_pt,\n",
    "                                       \"Jet_eta\": h.Jet_eta,\n",
    "                                       \"Jet_phi\": h.Jet_phi,\n",
    "                                       \"Jet_btagCSVV2\": h.Jet_btagCSVV2,\n",
    "                                       \"Jet_qgl\": h.Jet_qgl,\n",
    "                                       \"Jet_jetId\": h.Jet_jetId,\n",
    "                                       \"event\": h.event,\n",
    "                                      })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f08fc1",
   "metadata": {},
   "source": [
    "### Caching the queried datasets with `ServiceX`\n",
    "\n",
    "Using the queries created with `func_adl`, we are using `ServiceX` to read the CMS Open Data files to build cached files with only the specific event information as dictated by the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1d4f4ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": "{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}]",
       "colour": null,
       "elapsed": 0.003634214401245117,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "CMS ttbar",
       "rate": null,
       "total": 9000000000,
       "unit": "file",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eafdb99d27c14b7d9b27ce46b88a3853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CMS ttbar:   0%|          | 0/9000000000.0 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ServiceXException",
     "evalue": "(ServiceXException(...), 'Failed to get request errors for 9e070aa4-1088-4c56-9e96-918e18a42903: 404 - <!doctype html>\\n<html lang=en>\\n<title>404 Not Found</title>\\n<h1>Not Found</h1>\\n<p>The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.</p>\\n')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServiceXFailedFileTransform\u001b[0m               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/servicex/servicex.py:650\u001b[0m, in \u001b[0;36mServiceXDataset._stream_url_buckets\u001b[0;34m(self, selection_query, data_format, title, as_signed_url)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;66;03m# Reflect the files back up a level.\u001b[39;00m\n\u001b[0;32m--> 650\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m object_name \u001b[38;5;129;01min\u001b[39;00m minio_files:\n\u001b[1;32m    651\u001b[0m     uri \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    652\u001b[0m         minio_adaptor\u001b[38;5;241m.\u001b[39mget_access_url(request_id, object_name)\n\u001b[1;32m    653\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m as_signed_url\n\u001b[1;32m    654\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m minio_adaptor\u001b[38;5;241m.\u001b[39mget_s3_uri(request_id, object_name)\n\u001b[1;32m    655\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/servicex/servicex.py:1056\u001b[0m, in \u001b[0;36mServiceXDataset._get_minio_bucket_files_from_servicex\u001b[0;34m(self, request_id, client, minio_adaptor, notifier)\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;66;03m# Return the minio information.\u001b[39;00m\n\u001b[0;32m-> 1056\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m stream_new_object:\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m info\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/servicex/minio_adaptor.py:224\u001b[0m, in \u001b[0;36mfind_new_bucket_files\u001b[0;34m(adaptor, request_id, update)\u001b[0m\n\u001b[1;32m    223\u001b[0m seen \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 224\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m update:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# Sadly, this is blocking, and so may hold things up\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     files \u001b[38;5;241m=\u001b[39m cast(List[\u001b[38;5;28mstr\u001b[39m], adaptor\u001b[38;5;241m.\u001b[39mget_files(request_id))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/servicex/utils.py:242\u001b[0m, in \u001b[0;36mstream_unique_updates_only\u001b[0;34m(stream)\u001b[0m\n\u001b[1;32m    241\u001b[0m last_p: Optional[TransformTuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m stream:\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m!=\u001b[39m last_p:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/servicex/servicex_adaptor.py:270\u001b[0m, in \u001b[0;36mtrap_servicex_failures\u001b[0;34m(stream)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m did_fail \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m did_fail \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceXFailedFileTransform(\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServiceX failed to transform \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdid_fail\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles - data incomplete (remaining: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mremain\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprocessed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     )\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m p\n",
      "\u001b[0;31mServiceXFailedFileTransform\u001b[0m: (ServiceXFailedFileTransform(...), 'ServiceX failed to transform 1 files - data incomplete (remaining: 8, processed: 0).')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mServiceXException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     14\u001b[0m ds \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mServiceXDatasetGroup(fileset, backend_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muproot\u001b[39m\u001b[38;5;124m\"\u001b[39m, ignore_cache\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSERVICEX_IGNORE_CACHE\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 15\u001b[0m files_per_process \u001b[38;5;241m=\u001b[39m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_rootfiles_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_signed_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCMS ttbar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServiceX data delivery took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mt0\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# update fileset to point to ServiceX-transformed files\u001b[39;00m\n",
      "File \u001b[0;32m~/emk-fork/analysis-grand-challenge/analyses/cms-open-data-ttbar/utils/__init__.py:181\u001b[0m, in \u001b[0;36mServiceXDatasetGroup.get_data_rootfiles_uri\u001b[0;34m(self, query, as_signed_url, title)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data_rootfiles_uri\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, as_signed_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUntitled\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 181\u001b[0m     all_files \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_rootfiles_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_signed_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_signed_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtitle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    182\u001b[0m     parent_file_urls \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([f\u001b[38;5;241m.\u001b[39mfile \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m all_files])\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# order is not retained after transform, so we can match files to their parent files using the filename\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# (replacing / with : to mitigate servicex filename convention )\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/make_it_sync/func_wrapper.py:63\u001b[0m, in \u001b[0;36mmake_sync.<locals>.wrapped_call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sync_version_of_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/make_it_sync/func_wrapper.py:26\u001b[0m, in \u001b[0;36m_sync_version_of_function\u001b[0;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m exector \u001b[38;5;241m=\u001b[39m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     24\u001b[0m future \u001b[38;5;241m=\u001b[39m exector\u001b[38;5;241m.\u001b[39msubmit(get_data_wrapper, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/concurrent/futures/_base.py:444\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 444\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/concurrent/futures/_base.py:389\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 389\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/concurrent/futures/thread.py:57\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/make_it_sync/func_wrapper.py:21\u001b[0m, in \u001b[0;36m_sync_version_of_function.<locals>.get_data_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m asyncio\u001b[38;5;241m.\u001b[39mset_event_loop(loop)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mis_running()\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/asyncio/base_events.py:616\u001b[0m, in \u001b[0;36mBaseEventLoop.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m future\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 616\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/servicex/servicex.py:520\u001b[0m, in \u001b[0;36mServiceXDataset.get_data_rootfiles_uri_async\u001b[0;34m(self, selection_query, title, as_signed_url)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data_rootfiles_uri_async\u001b[39m(\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    506\u001b[0m     selection_query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    507\u001b[0m     title: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    508\u001b[0m     as_signed_url: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    509\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[StreamInfoUrl]:\n\u001b[1;32m    510\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of StreamInfoUrl entries containing a `url` for each output file\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03m    from the transform, taken from get_data_rootfiles_uri_stream.\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;124;03m    The data that comes back includes a `url` that can be accessed to download the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;124;03m        as_signed_url (bool): Return the uri as a presigned http url?\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    521\u001b[0m         f\n\u001b[1;32m    522\u001b[0m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_data_rootfiles_uri_stream(\n\u001b[1;32m    523\u001b[0m             selection_query, title\u001b[38;5;241m=\u001b[39mtitle, as_signed_url\u001b[38;5;241m=\u001b[39mas_signed_url\n\u001b[1;32m    524\u001b[0m         )\n\u001b[1;32m    525\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/servicex/servicex.py:520\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data_rootfiles_uri_async\u001b[39m(\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    506\u001b[0m     selection_query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    507\u001b[0m     title: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    508\u001b[0m     as_signed_url: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    509\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[StreamInfoUrl]:\n\u001b[1;32m    510\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a list of StreamInfoUrl entries containing a `url` for each output file\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03m    from the transform, taken from get_data_rootfiles_uri_stream.\u001b[39;00m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;124;03m    The data that comes back includes a `url` that can be accessed to download the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;124;03m        as_signed_url (bool): Return the uri as a presigned http url?\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    521\u001b[0m         f\n\u001b[1;32m    522\u001b[0m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_data_rootfiles_uri_stream(\n\u001b[1;32m    523\u001b[0m             selection_query, title\u001b[38;5;241m=\u001b[39mtitle, as_signed_url\u001b[38;5;241m=\u001b[39mas_signed_url\n\u001b[1;32m    524\u001b[0m         )\n\u001b[1;32m    525\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/servicex/servicex.py:499\u001b[0m, in \u001b[0;36mServiceXDataset.get_data_rootfiles_uri_stream\u001b[0;34m(self, selection_query, title, as_signed_url)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data_rootfiles_uri_stream\u001b[39m(\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    486\u001b[0m     selection_query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    487\u001b[0m     title: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    488\u001b[0m     as_signed_url: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    489\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[StreamInfoUrl]:\n\u001b[1;32m    490\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns, as an async iterator, each completed batch of work from ServiceX.\u001b[39;00m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;124;03m    The data that comes back includes a `url` that can be accessed to download the\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;124;03m    data.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;124;03m        as_signed_url (bool): Return the uri as a presigned http url?\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m f_info \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream_url_buckets(\n\u001b[1;32m    500\u001b[0m         selection_query, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot-file\u001b[39m\u001b[38;5;124m\"\u001b[39m, title, as_signed_url\n\u001b[1;32m    501\u001b[0m     ):  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    502\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f_info\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/servicex/utils.py:602\u001b[0m, in \u001b[0;36mretry_exception_itr.<locals>.retry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m got_one_item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 602\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    603\u001b[0m         got_one_item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/servicex/utils.py:602\u001b[0m, in \u001b[0;36mretry_exception_itr.<locals>.retry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m got_one_item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 602\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    603\u001b[0m         got_one_item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/servicex/servicex.py:682\u001b[0m, in \u001b[0;36mServiceXDataset._stream_url_buckets\u001b[0;34m(self, selection_query, data_format, title, as_signed_url)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ServiceXFailedFileTransform \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mremove_query(query)\n\u001b[0;32m--> 682\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_servicex_adaptor\u001b[38;5;241m.\u001b[39mdump_query_errors(client, request_id)\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceXException(\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to transform all files in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    685\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/servicex/servicex_adaptor.py:134\u001b[0m, in \u001b[0;36mServiceXAdaptor.dump_query_errors\u001b[0;34m(self, client, request_id)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ServiceXUnknownRequestID(\n\u001b[1;32m    130\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to get errors for request \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m         )\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 134\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ServiceXException(\n\u001b[1;32m    135\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to get request errors for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    136\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m         )\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Dump the messages out to the logger if there are any!\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Keep the output under control - only dump the first 20 errors (see #188)\u001b[39;00m\n\u001b[1;32m    141\u001b[0m errors \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mawait\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson())[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mServiceXException\u001b[0m: (ServiceXException(...), 'Failed to get request errors for 9e070aa4-1088-4c56-9e96-918e18a42903: 404 - <!doctype html>\\n<html lang=en>\\n<title>404 Not Found</title>\\n<h1>Not Found</h1>\\n<p>The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.</p>\\n')"
     ]
    }
   ],
   "source": [
    "if USE_SERVICEX:\n",
    "    # dummy dataset on which to generate the query\n",
    "    dummy_ds = ServiceXSourceUpROOT(\"cernopendata://dummy\", \"Events\", backend_name=\"uproot\")\n",
    "\n",
    "    # tell low-level infrastructure not to contact ServiceX yet, only to\n",
    "    # return the qastle string it would have sent\n",
    "    dummy_ds.return_qastle = True\n",
    "\n",
    "    # create the query\n",
    "    query = get_query(dummy_ds).value()\n",
    "\n",
    "    # now we query the files using a wrapper around ServiceXDataset to transform all processes at once\n",
    "    t0 = time.time()\n",
    "    ds = utils.ServiceXDatasetGroup(fileset, backend_name=\"uproot\", ignore_cache=config[\"global\"][\"SERVICEX_IGNORE_CACHE\"])\n",
    "    files_per_process = ds.get_data_rootfiles_uri(query, as_signed_url=True, title=\"CMS ttbar\")\n",
    "\n",
    "    print(f\"ServiceX data delivery took {time.time() - t0:.2f} seconds\")\n",
    "\n",
    "    # update fileset to point to ServiceX-transformed files\n",
    "    for process in fileset.keys():\n",
    "        fileset[process][\"files\"] = [f.url for f in files_per_process[process]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28a9e49",
   "metadata": {},
   "source": [
    "### Execute the data delivery pipeline\n",
    "\n",
    "What happens here depends on the flag `USE_SERVICEX`. If set to true, the processor is run on the data previously gathered by ServiceX, then will gather output histograms.\n",
    "\n",
    "When `USE_SERVICEX` is false, the input files need to be processed during this step as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59d30d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NanoAODSchema.warn_missing_crossrefs = False # silences warnings about branches we will not use here\n",
    "if USE_DASK:\n",
    "    executor = processor.DaskExecutor(client=utils.get_client(af=config[\"global\"][\"AF\"]))\n",
    "else:\n",
    "    executor = processor.FuturesExecutor(workers=config[\"benchmarking\"][\"NUM_CORES\"])\n",
    "\n",
    "run = processor.Runner(executor=executor, schema=NanoAODSchema, savemetrics=True, metadata_cache={}, chunksize=config[\"benchmarking\"][\"CHUNKSIZE\"])\n",
    "\n",
    "if USE_SERVICEX:\n",
    "    treename = \"servicex\"\n",
    "\n",
    "else:\n",
    "    treename = \"Events\"\n",
    "\n",
    "if not USE_DASK and not USE_TRITON and USE_INFERENCE:\n",
    "    model_even = config[\"ml\"][\"XGBOOST_MODEL_PATH_EVEN\"]\n",
    "    model_odd = config[\"ml\"][\"XGBOOST_MODEL_PATH_ODD\"]\n",
    "\n",
    "elif not USE_TRITON and USE_INFERENCE:\n",
    "    model_even = XGBClassifier()\n",
    "    model_even.load_model(config[\"ml\"][\"XGBOOST_MODEL_PATH_EVEN\"])\n",
    "    model_odd = XGBClassifier()\n",
    "    model_odd.load_model(config[\"ml\"][\"XGBOOST_MODEL_PATH_ODD\"])\n",
    "\n",
    "else:\n",
    "    model_even = None\n",
    "    model_odd = None\n",
    "\n",
    "filemeta = run.preprocess(fileset, treename=treename)  # pre-processing\n",
    "\n",
    "t0 = time.monotonic()\n",
    "all_histograms, metrics = run(fileset, treename, processor_instance=TtbarAnalysis(USE_DASK,\n",
    "                                                                                  config[\"benchmarking\"][\"DISABLE_PROCESSING\"],\n",
    "                                                                                  config[\"benchmarking\"][\"IO_BRANCHES\"][\n",
    "                                                                                      config[\"benchmarking\"][\"IO_FILE_PERCENT\"]\n",
    "                                                                                  ],\n",
    "                                                                                  config[\"ml\"],\n",
    "                                                                                  model_even,\n",
    "                                                                                  model_odd,\n",
    "                                                                                  permutations_dict))  # processing\n",
    "exec_time = time.monotonic() - t0\n",
    "\n",
    "print(f\"\\nexecution took {exec_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f0dc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# track metrics\n",
    "dataset_source = \"/data\" if fileset[\"ttbar__nominal\"][\"files\"][0].startswith(\"/data\") else \"https://xrootd-local.unl.edu:1094\" # TODO: xcache support\n",
    "metrics.update({\n",
    "    \"walltime\": exec_time,\n",
    "    \"num_workers\": config[\"benchmarking\"][\"NUM_CORES\"],\n",
    "    \"af\": config[\"benchmarking\"][\"AF_NAME\"],\n",
    "    \"dataset_source\": dataset_source,\n",
    "    \"use_dask\": USE_DASK,\n",
    "    \"use_servicex\": USE_SERVICEX,\n",
    "    \"systematics\": config[\"benchmarking\"][\"SYSTEMATICS\"],\n",
    "    \"n_files_max_per_sample\": N_FILES_MAX_PER_SAMPLE,\n",
    "    \"cores_per_worker\": config[\"benchmarking\"][\"CORES_PER_WORKER\"],\n",
    "    \"chunksize\": config[\"benchmarking\"][\"CHUNKSIZE\"],\n",
    "    \"disable_processing\": config[\"benchmarking\"][\"DISABLE_PROCESSING\"],\n",
    "    \"io_file_percent\": config[\"benchmarking\"][\"IO_FILE_PERCENT\"]\n",
    "})\n",
    "\n",
    "# save metrics to disk\n",
    "if not os.path.exists(\"metrics\"):\n",
    "    os.makedirs(\"metrics\")\n",
    "timestamp = time.strftime('%Y%m%d-%H%M%S')\n",
    "af_name = metrics[\"af\"]\n",
    "metric_file_name = f\"metrics/{af_name}-{timestamp}.json\"\n",
    "with open(metric_file_name, \"w\") as f:\n",
    "    f.write(json.dumps(metrics))\n",
    "\n",
    "print(f\"metrics saved as {metric_file_name}\")\n",
    "#print(f\"event rate per worker (full execution time divided by NUM_CORES={NUM_CORES}): {metrics['entries'] / NUM_CORES / exec_time / 1_000:.2f} kHz\")\n",
    "print(f\"event rate per worker (pure processtime): {metrics['entries'] / metrics['processtime'] / 1_000:.2f} kHz\")\n",
    "print(f\"amount of data read: {metrics['bytesread']/1000**2:.2f} MB\")  # likely buggy: https://github.com/CoffeaTeam/coffea/issues/717"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bb4428",
   "metadata": {},
   "source": [
    "### Inspecting the produced histograms\n",
    "\n",
    "Let's have a look at the data we obtained.\n",
    "We built histograms in two phase space regions, for multiple physics processes and systematic variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd348fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.set_style()\n",
    "\n",
    "all_histograms[\"hist\"][120j::hist.rebin(2), \"4j1b\", :, \"nominal\"].stack(\"process\")[::-1].plot(stack=True, histtype=\"fill\", linewidth=1, edgecolor=\"grey\")\n",
    "plt.legend(frameon=False)\n",
    "plt.title(\"$\\geq$ 4 jets, 1 b-tag\")\n",
    "plt.xlabel(\"$H_T$ [GeV]\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c902d85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_histograms[\"hist\"][:, \"4j2b\", :, \"nominal\"].stack(\"process\")[::-1].plot(stack=True, histtype=\"fill\", linewidth=1,edgecolor=\"grey\")\n",
    "plt.legend(frameon=False)\n",
    "plt.title(\"$\\geq$ 4 jets, $\\geq$ 2 b-tags\")\n",
    "plt.xlabel(\"$m_{bjj}$ [GeV]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed3df8b",
   "metadata": {},
   "source": [
    "Our top reconstruction approach ($bjj$ system with largest $p_T$) has worked!\n",
    "\n",
    "Let's also have a look at some systematic variations:\n",
    "- b-tagging, which we implemented as jet-kinematic dependent event weights,\n",
    "- jet energy variations, which vary jet kinematics, resulting in acceptance effects and observable changes.\n",
    "\n",
    "We are making of [UHI](https://uhi.readthedocs.io/) here to re-bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1aabfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# b-tagging variations\n",
    "all_histograms[\"hist\"][120j::hist.rebin(2), \"4j1b\", \"ttbar\", \"nominal\"].plot(label=\"nominal\", linewidth=2)\n",
    "all_histograms[\"hist\"][120j::hist.rebin(2), \"4j1b\", \"ttbar\", \"btag_var_0_up\"].plot(label=\"NP 1\", linewidth=2)\n",
    "all_histograms[\"hist\"][120j::hist.rebin(2), \"4j1b\", \"ttbar\", \"btag_var_1_up\"].plot(label=\"NP 2\", linewidth=2)\n",
    "all_histograms[\"hist\"][120j::hist.rebin(2), \"4j1b\", \"ttbar\", \"btag_var_2_up\"].plot(label=\"NP 3\", linewidth=2)\n",
    "all_histograms[\"hist\"][120j::hist.rebin(2), \"4j1b\", \"ttbar\", \"btag_var_3_up\"].plot(label=\"NP 4\", linewidth=2)\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel(\"$H_T$ [GeV]\")\n",
    "plt.title(\"b-tagging variations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f89e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# jet energy scale variations\n",
    "all_histograms[\"hist\"][:, \"4j2b\", \"ttbar\", \"nominal\"].plot(label=\"nominal\", linewidth=2)\n",
    "all_histograms[\"hist\"][:, \"4j2b\", \"ttbar\", \"pt_scale_up\"].plot(label=\"scale up\", linewidth=2)\n",
    "all_histograms[\"hist\"][:, \"4j2b\", \"ttbar\", \"pt_res_up\"].plot(label=\"resolution up\", linewidth=2)\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel(\"$m_{bjj}$ [Gev]\")\n",
    "plt.title(\"Jet energy variations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa616ac",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ML inference variables\n",
    "if USE_INFERENCE:\n",
    "    fig, axs = plt.subplots(10,2,figsize=(14,40))\n",
    "    for i in range(len(config[\"ml\"][\"FEATURE_NAMES\"])):\n",
    "        if i<10: \n",
    "            column=0\n",
    "            row=i\n",
    "        else: \n",
    "            column=1\n",
    "            row=i-10\n",
    "        all_histograms['ml_hist_dict'][f'hist_{config[\"ml\"][\"FEATURE_NAMES\"][i]}'][:, :, \"nominal\"].stack(\"process\").project(\"observable\").plot(\n",
    "            stack=True, \n",
    "            histtype=\"fill\", \n",
    "            linewidth=1, \n",
    "            edgecolor=\"grey\", \n",
    "            ax=axs[row,column])\n",
    "        axs[row, column].legend(frameon=False)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c334dd3",
   "metadata": {},
   "source": [
    "### Save histograms to disk\n",
    "\n",
    "We'll save everything to disk for subsequent usage.\n",
    "This also builds pseudo-data by combining events from the various simulation setups we have processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4d05ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.save_histograms(all_histograms['hist'], fileset, \"histograms.root\")\n",
    "if USE_INFERENCE:\n",
    "    utils.save_ml_histograms(all_histograms['ml_hist_dict'], fileset, \"histograms_ml.root\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e904cd3c",
   "metadata": {},
   "source": [
    "### Statistical inference\n",
    "\n",
    "A statistical model has been defined in `config.yml`, ready to be used with our output.\n",
    "We will use `cabinetry` to combine all histograms into a `pyhf` workspace and fit the resulting statistical model to the pseudodata we built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b89fb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = cabinetry.configuration.load(\"cabinetry_config.yml\")\n",
    "cabinetry.templates.collect(config)\n",
    "cabinetry.templates.postprocess(config)  # optional post-processing (e.g. smoothing)\n",
    "ws = cabinetry.workspace.build(config)\n",
    "cabinetry.workspace.save(ws, \"workspace.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36dc601",
   "metadata": {},
   "source": [
    "We can inspect the workspace with `pyhf`, or use `pyhf` to perform inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a83712",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pyhf inspect workspace.json | head -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74e4361",
   "metadata": {},
   "source": [
    "Let's try out what we built: the next cell will perform a maximum likelihood fit of our statistical model to the pseudodata we built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358d17dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, data = cabinetry.model_utils.model_and_data(ws)\n",
    "fit_results = cabinetry.fit.fit(model, data)\n",
    "\n",
    "cabinetry.visualize.pulls(\n",
    "    fit_results, exclude=\"ttbar_norm\", close_figure=True, save_figure=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd480eec",
   "metadata": {},
   "source": [
    "For this pseudodata, what is the resulting ttbar cross-section divided by the Standard Model prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8ffdf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "poi_index = model.config.poi_index\n",
    "print(f\"\\nfit result for ttbar_norm: {fit_results.bestfit[poi_index]:.3f} +/- {fit_results.uncertainty[poi_index]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a293479",
   "metadata": {},
   "source": [
    "Let's also visualize the model before and after the fit, in both the regions we are using.\n",
    "The binning here corresponds to the binning used for the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfab0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prediction = cabinetry.model_utils.prediction(model)\n",
    "figs = cabinetry.visualize.data_mc(model_prediction, data, close_figure=True, config=config)\n",
    "figs[0][\"figure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e20f9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "figs[1][\"figure\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107409bc",
   "metadata": {},
   "source": [
    "We can see very good post-fit agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9c1607",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_prediction_postfit = cabinetry.model_utils.prediction(model, fit_results=fit_results)\n",
    "figs = cabinetry.visualize.data_mc(model_prediction_postfit, data, close_figure=True, config=config)\n",
    "figs[0][\"figure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aee420a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "figs[1][\"figure\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dc4b23",
   "metadata": {},
   "source": [
    "### ML Validation\n",
    "We can further validate our results by applying the above fit to different ML observables and checking for good agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84c7e1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the ml workspace (uses the ml observable instead of previous method)\n",
    "if USE_INFERENCE:\n",
    "    config_ml = cabinetry.configuration.load(\"cabinetry_config_ml.yml\")\n",
    "    cabinetry.templates.collect(config_ml)\n",
    "    cabinetry.templates.postprocess(config_ml)  # optional post-processing (e.g. smoothing)\n",
    "\n",
    "    ws_ml = cabinetry.workspace.build(config_ml)\n",
    "    ws_pruned = pyhf.Workspace(ws_ml).prune(channels=[\"Feature3\", \"Feature8\", \"Feature9\",\n",
    "                                                      \"Feature10\", \"Feature11\", \"Feature12\",\n",
    "                                                      \"Feature13\", \"Feature14\", \"Feature15\",\n",
    "                                                      \"Feature16\", \"Feature17\", \"Feature18\",\n",
    "                                                      \"Feature19\"])\n",
    "\n",
    "    cabinetry.workspace.save(ws_pruned, \"workspace_ml.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bad987f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if USE_INFERENCE:\n",
    "    model_ml, data_ml = cabinetry.model_utils.model_and_data(ws_pruned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f60c316",
   "metadata": {},
   "source": [
    "We have a channel for each ML observable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e36bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pyhf inspect workspace_ml.json | head -n 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554c32dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain model prediction before and after fit\n",
    "if USE_INFERENCE:\n",
    "    model_prediction = cabinetry.model_utils.prediction(model_ml)\n",
    "    fit_results_mod = cabinetry.model_utils.match_fit_results(model_ml, fit_results)\n",
    "    model_prediction_postfit = cabinetry.model_utils.prediction(model_ml, fit_results=fit_results_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee708250",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if USE_INFERENCE:\n",
    "    figs = utils.plot_data_mc(model_prediction, model_prediction_postfit, data_ml, config_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ce2d14",
   "metadata": {},
   "source": [
    "### What is next?\n",
    "\n",
    "Our next goals for this pipeline demonstration are:\n",
    "- making this analysis even **more feature-complete**,\n",
    "- **addressing performance bottlenecks** revealed by this demonstrator,\n",
    "- **collaborating** with you!\n",
    "\n",
    "Please do not hesitate to get in touch if you would like to join the effort, or are interested in re-implementing (pieces of) the pipeline with different tools!\n",
    "\n",
    "Our mailing list is analysis-grand-challenge@iris-hep.org, sign up via the [Google group](https://groups.google.com/a/iris-hep.org/g/analysis-grand-challenge)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
